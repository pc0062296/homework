{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas \n",
    "import random \n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pandas.read_csv(\"ionosphere_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1  0  0.99539  -0.05889  0.85243  0.02306  0.83398  -0.37708      1.1  \\\n",
      "0  1  0  1.00000  -0.18829  0.93035 -0.36156 -0.10868  -0.93597  1.00000   \n",
      "1  1  0  1.00000  -0.03365  1.00000  0.00485  1.00000  -0.12062  0.88965   \n",
      "2  1  0  1.00000  -0.45161  1.00000  1.00000  0.71216  -1.00000  0.00000   \n",
      "3  1  0  1.00000  -0.02401  0.94140  0.06531  0.92106  -0.23255  0.77152   \n",
      "4  1  0  0.02337  -0.00592 -0.09924 -0.11949 -0.00763  -0.11824  0.14706   \n",
      "\n",
      "   0.03760  ...  -0.51171  0.41078  -0.46168  0.21266  -0.34090  0.42267  \\\n",
      "0 -0.04549  ...  -0.26569 -0.20468  -0.18401 -0.19040  -0.11593 -0.16626   \n",
      "1  0.01198  ...  -0.40220  0.58984  -0.22145  0.43100  -0.17365  0.60436   \n",
      "2  0.00000  ...   0.90695  0.51613   1.00000  1.00000  -0.20099  0.25682   \n",
      "3 -0.16399  ...  -0.65158  0.13290  -0.53206  0.02431  -0.62197 -0.05707   \n",
      "4  0.06637  ...  -0.01535 -0.03240   0.09223 -0.07859   0.00732  0.00000   \n",
      "\n",
      "   -0.54487  0.18641  -0.45300  g  \n",
      "0  -0.06288 -0.13738  -0.02447  b  \n",
      "1  -0.24180  0.56045  -0.38238  g  \n",
      "2   1.00000 -0.32382   1.00000  b  \n",
      "3  -0.59573 -0.04608  -0.65697  g  \n",
      "4   0.00000 -0.00039   0.12011  b  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                1      0     0.99539    -0.05889     0.85243     0.02306  \\\n",
      "count  350.000000  350.0  350.000000  350.000000  350.000000  350.000000   \n",
      "mean     0.891429    0.0    0.640330    0.044667    0.600350    0.116154   \n",
      "std      0.311546    0.0    0.498059    0.442032    0.520431    0.461443   \n",
      "min      0.000000    0.0   -1.000000   -1.000000   -1.000000   -1.000000   \n",
      "25%      1.000000    0.0    0.471518   -0.065388    0.412555   -0.024868   \n",
      "50%      1.000000    0.0    0.870795    0.016700    0.808620    0.021170   \n",
      "75%      1.000000    0.0    1.000000    0.194727    1.000000    0.335318   \n",
      "max      1.000000    0.0    1.000000    1.000000    1.000000    1.000000   \n",
      "\n",
      "          0.83398    -0.37708         1.1     0.03760  ...     0.56811  \\\n",
      "count  350.000000  350.000000  350.000000  350.000000  ...  350.000000   \n",
      "mean     0.549284    0.120779    0.510453    0.181756  ...    0.395643   \n",
      "std      0.493124    0.520816    0.507117    0.484482  ...    0.579206   \n",
      "min     -1.000000   -1.000000   -1.000000   -1.000000  ...   -1.000000   \n",
      "25%      0.209105   -0.053483    0.086785   -0.049003  ...    0.000000   \n",
      "50%      0.728000    0.015085    0.682430    0.017550  ...    0.549175   \n",
      "75%      0.970445    0.451572    0.950555    0.536192  ...    0.907165   \n",
      "max      1.000000    1.000000    1.000000    1.000000  ...    1.000000   \n",
      "\n",
      "         -0.51171     0.41078    -0.46168     0.21266    -0.34090     0.42267  \\\n",
      "count  350.000000  350.000000  350.000000  350.000000  350.000000  350.000000   \n",
      "mean    -0.069928    0.542015   -0.068417    0.378919   -0.027013    0.352313   \n",
      "std      0.508675    0.516896    0.550411    0.576642    0.508425    0.572289   \n",
      "min     -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   \n",
      "25%     -0.323745    0.283612   -0.428992    0.000000   -0.234935    0.000000   \n",
      "50%     -0.014915    0.708530   -0.017685    0.499215    0.000000    0.446875   \n",
      "75%      0.157922    0.999972    0.154862    0.884572    0.154218    0.859490   \n",
      "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
      "\n",
      "         -0.54487     0.18641    -0.45300  \n",
      "count  350.000000  350.000000  350.000000  \n",
      "mean    -0.002248    0.349829    0.015816  \n",
      "std      0.513491    0.523339    0.468338  \n",
      "min     -1.000000   -1.000000   -1.000000  \n",
      "25%     -0.239347    0.000000   -0.161013  \n",
      "50%      0.000000    0.413115    0.000000  \n",
      "75%      0.200935    0.816777    0.172105  \n",
      "max      1.000000    1.000000    1.000000  \n",
      "\n",
      "[8 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataframe.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (350, 35)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape:\", dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types: 1              int64\n",
      "0              int64\n",
      "0.99539      float64\n",
      "-0.05889     float64\n",
      "0.85243      float64\n",
      "0.02306      float64\n",
      "0.83398      float64\n",
      "-0.37708     float64\n",
      "1.1          float64\n",
      "0.03760      float64\n",
      "0.85243.1    float64\n",
      "-0.17755     float64\n",
      "0.59755      float64\n",
      "-0.44945     float64\n",
      "0.60536      float64\n",
      "-0.38223     float64\n",
      "0.84356      float64\n",
      "-0.38542     float64\n",
      "0.58212      float64\n",
      "-0.32192     float64\n",
      "0.56971      float64\n",
      "-0.29674     float64\n",
      "0.36946      float64\n",
      "-0.47357     float64\n",
      "0.56811      float64\n",
      "-0.51171     float64\n",
      "0.41078      float64\n",
      "-0.46168     float64\n",
      "0.21266      float64\n",
      "-0.34090     float64\n",
      "0.42267      float64\n",
      "-0.54487     float64\n",
      "0.18641      float64\n",
      "-0.45300     float64\n",
      "g             object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Types:\", dataframe.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1.0 ... -0.13738 -0.02447 'b']\n",
      " [1 0 1.0 ... 0.56045 -0.38238 'g']\n",
      " [1 0 1.0 ... -0.32382 1.0 'b']\n",
      " ...\n",
      " [1 0 0.94701 ... 0.9269700000000001 -0.00577 'g']\n",
      " [1 0 0.9060799999999999 ... 0.87403 -0.16243 'g']\n",
      " [1 0 0.8471 ... 0.85764 -0.06151 'g']]\n"
     ]
    }
   ],
   "source": [
    "print(dataframe.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0.0, ..., -0.19792, -0.9375, 'b'],\n",
       "       [1, 0, 0.89835, ..., 0.42805, 0.13297, 'g'],\n",
       "       [1, 0, 0.7381, ..., 0.2619, 0.023809999999999998, 'b'],\n",
       "       ...,\n",
       "       [1, 0, 1.0, ..., 0.0, 0.0, 'b'],\n",
       "       [1, 0, -1.0, ..., 0.0, 0.0, 'b'],\n",
       "       [1, 0, 1.0, ..., 0.9652299999999999, -0.11717000000000001, 'g']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = np.array(dataframe.values)\n",
    "dataset_shuf = []\n",
    "index_shuf = list(range(len(dataset)))\n",
    "random.shuffle(index_shuf)\n",
    "for i in index_shuf:\n",
    "    dataset_shuf.append(dataset[i,:])   \n",
    "dataset_shuf = np.array(dataset_shuf)\n",
    "dataset_shuf.reshape(350,35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['g']], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(dataset_shuf[:280,0:34], dtype = np.float)\n",
    "Y_train = np.array(dataset_shuf[:280,34:35], dtype = np.object)\n",
    "X_train.reshape(280,34)\n",
    "Y_train.reshape(280,1)\n",
    "\n",
    "X_test = np.array(dataset_shuf[280:,0:34], dtype = np.float)\n",
    "Y_test = np.array(dataset_shuf[280:,34:35], dtype = np.object)\n",
    "X_test.reshape(70,34)\n",
    "Y_test.reshape(70,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "\n",
    "Y_train[:,0] = labelencoder.fit_transform(Y_train[:, 0])\n",
    "Y_test[:,0] = labelencoder.fit_transform(Y_test[:, 0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.     ,  0.     ,  0.     , ..., -0.27083, -0.19792, -0.9375 ],\n",
       "       [ 1.     ,  0.     ,  0.89835, ..., -0.30057,  0.42805,  0.13297],\n",
       "       [ 1.     ,  0.     ,  0.7381 , ..., -0.04762,  0.2619 ,  0.02381],\n",
       "       ...,\n",
       "       [ 0.     ,  0.     ,  0.     , ...,  1.     ,  0.     ,  0.     ],\n",
       "       [ 1.     ,  0.     ,  0.62335, ..., -0.00963,  0.61372, -0.09146],\n",
       "       [ 1.     ,  0.     ,  1.     , ..., -1.     ,  1.     , -1.     ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.DataFrame(X_train)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define weight:\n",
    "\n",
    "#7 for hidden layer1\n",
    "#5 for hidden layer2\n",
    "#2 for output layer\n",
    "#9 total\n",
    "\n",
    "weight_hidden = np.random.random((34,68))-0.5\n",
    "weight_hidden2 = np.random.random((68,34))-0.5\n",
    "weight_hidden3 = np.random.random((34,34))-0.5\n",
    "weight_hidden4 = np.random.random((34,34))-0.5\n",
    "weight_hidden5 = np.random.random((34,2))-0.5\n",
    "weight_output = np.random.random((2,1))-0.5\n",
    "lr = 0.001\n",
    "bias = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x) :\n",
    "    return x * (x > 0) + 0.01 * x * (x <= 0)\n",
    "\n",
    "def ReLU_der(x) :\n",
    "    return 1 * (x > 0) + 0.01 * (x <= 0)\n",
    "\n",
    "def Sigmoid(x) :\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def Sigmoid_der(x) :\n",
    "    return Sigmoid(x) * (1 - Sigmoid(x))\n",
    "\n",
    "def limit(x) :  \n",
    "    while(np.max(x) > 0.1 or np.min(x) < -0.1) : \n",
    "            x /= 10\n",
    "    return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now training  0.0 %\n",
      "93.140519087185\n",
      "now training  0.1 %\n",
      "91.76466680567914\n",
      "now training  0.2 %\n",
      "41.47196891381543\n",
      "now training  0.3 %\n",
      "37.2067847392029\n",
      "now training  0.4 %\n",
      "34.87800057780335\n",
      "now training  0.5 %\n",
      "34.24460905580223\n",
      "now training  0.6 %\n",
      "34.78823911584535\n",
      "now training  0.7 %\n",
      "34.54338270198951\n",
      "now training  0.8 %\n",
      "37.59844869055355\n",
      "now training  0.9 %\n",
      "36.93111489160158\n",
      "now training  1.0 %\n",
      "37.41120211786223\n",
      "now training  1.1 %\n",
      "37.27818741177775\n",
      "now training  1.2 %\n",
      "38.255035818100055\n",
      "now training  1.3 %\n",
      "40.40401338946653\n",
      "now training  1.4 %\n",
      "41.011698136832315\n",
      "now training  1.5 %\n",
      "39.86233816012951\n",
      "now training  1.6 %\n",
      "41.24008603006887\n",
      "now training  1.7 %\n",
      "43.74916370550664\n",
      "now training  1.8 %\n",
      "43.39258501511609\n",
      "now training  1.9 %\n",
      "44.06395161212969\n",
      "now training  2.0 %\n",
      "43.77636857793251\n",
      "now training  2.1 %\n",
      "45.388078409653524\n",
      "now training  2.2 %\n",
      "46.34600716574208\n",
      "now training  2.3 %\n",
      "47.22866812580261\n",
      "now training  2.4 %\n",
      "49.65902591511426\n",
      "now training  2.5 %\n",
      "48.525716661972126\n",
      "now training  2.6 %\n",
      "49.075921521518794\n",
      "now training  2.7 %\n",
      "49.23392160158111\n",
      "now training  2.8 %\n",
      "49.905349252437496\n",
      "now training  2.9 %\n",
      "52.38094811455506\n",
      "now training  3.0 %\n",
      "53.22336659298951\n",
      "now training  3.1 %\n",
      "54.86693621468409\n",
      "now training  3.2 %\n",
      "54.64499972676286\n",
      "now training  3.3 %\n",
      "55.25191160255879\n",
      "now training  3.4 %\n",
      "55.57054537884188\n",
      "now training  3.5 %\n",
      "57.0269161308988\n",
      "now training  3.6 %\n",
      "57.732640624091566\n",
      "now training  3.7 %\n",
      "57.53476182051308\n",
      "now training  3.8 %\n",
      "58.28761554730814\n",
      "now training  3.9 %\n",
      "59.25171247217499\n",
      "now training  4.0 %\n",
      "58.27457496112886\n",
      "now training  4.1 %\n",
      "59.77493491462866\n",
      "now training  4.2 %\n",
      "60.219503953538656\n",
      "now training  4.3 %\n",
      "61.5608899829469\n",
      "now training  4.4 %\n",
      "62.42698283612297\n",
      "now training  4.5 %\n",
      "61.6622625582425\n",
      "now training  4.6 %\n",
      "62.0621201057895\n",
      "now training  4.7 %\n",
      "63.13413930344957\n",
      "now training  4.8 %\n",
      "63.60133308329473\n",
      "now training  4.9 %\n",
      "63.837087361898654\n",
      "now training  5.0 %\n",
      "63.79722490566825\n",
      "now training  5.1 %\n",
      "64.40237150714042\n",
      "now training  5.2 %\n",
      "64.23201994876696\n",
      "now training  5.3 %\n",
      "64.15211409305246\n",
      "now training  5.4 %\n",
      "64.6967761442377\n",
      "now training  5.5 %\n",
      "64.87274153489238\n",
      "now training  5.6 %\n",
      "65.21934609847627\n",
      "now training  5.7 %\n",
      "66.26210562534918\n",
      "now training  5.8 %\n",
      "66.15848853825946\n",
      "now training  5.9 %\n",
      "66.55137599810989\n",
      "now training  6.0 %\n",
      "66.22618974636566\n",
      "now training  6.1 %\n",
      "66.26217770858328\n",
      "now training  6.2 %\n",
      "66.75052612619025\n",
      "now training  6.3 %\n",
      "66.7417143403355\n",
      "now training  6.4 %\n",
      "67.438478865563\n",
      "now training  6.5 %\n",
      "67.15537761189844\n",
      "now training  6.6 %\n",
      "67.96908824418877\n",
      "now training  6.7 %\n",
      "67.94797605371053\n",
      "now training  6.8 %\n",
      "68.01829656400906\n",
      "now training  6.9 %\n",
      "68.17345883112432\n",
      "now training  7.0 %\n",
      "68.20213648293294\n",
      "now training  7.1 %\n",
      "68.30626764258413\n",
      "now training  7.2 %\n",
      "68.70583256062018\n",
      "now training  7.3 %\n",
      "69.03892660783167\n",
      "now training  7.4 %\n",
      "68.75800796326868\n",
      "now training  7.5 %\n",
      "68.60706976153176\n",
      "now training  7.6 %\n",
      "68.98301576380233\n",
      "now training  7.7 %\n",
      "68.82383898734682\n",
      "now training  7.8 %\n",
      "68.92521702009317\n",
      "now training  7.9 %\n",
      "69.03618369122448\n",
      "now training  8.0 %\n",
      "69.48989550342013\n",
      "now training  8.1 %\n",
      "69.55305276536393\n",
      "now training  8.2 %\n",
      "69.12705143230849\n",
      "now training  8.3 %\n",
      "69.2169504155005\n",
      "now training  8.4 %\n",
      "69.96606380310526\n",
      "now training  8.5 %\n",
      "69.45988638587859\n",
      "now training  8.6 %\n",
      "69.75470151991733\n",
      "now training  8.7 %\n",
      "69.70114862587008\n",
      "now training  8.8 %\n",
      "69.7515157802057\n",
      "now training  8.9 %\n",
      "69.9802762939325\n",
      "now training  9.0 %\n",
      "70.1335363146696\n",
      "now training  9.1 %\n",
      "70.50268283787324\n",
      "now training  9.2 %\n",
      "70.30064427053989\n",
      "now training  9.3 %\n",
      "70.64288858894368\n",
      "now training  9.4 %\n",
      "70.59868030663849\n",
      "now training  9.5 %\n",
      "70.42444919582044\n",
      "now training  9.6 %\n",
      "70.9982177115163\n",
      "now training  9.7 %\n",
      "70.74581578263418\n",
      "now training  9.8 %\n",
      "70.72404702693699\n",
      "now training  9.9 %\n",
      "71.03154713987776\n",
      "now training  10.0 %\n",
      "70.93726248700976\n",
      "now training  10.1 %\n",
      "71.22468838652769\n",
      "now training  10.2 %\n",
      "71.04716333618782\n",
      "now training  10.3 %\n",
      "71.51210788920626\n",
      "now training  10.4 %\n",
      "71.58104334158003\n",
      "now training  10.5 %\n",
      "71.33699135578\n",
      "now training  10.6 %\n",
      "71.54541608693769\n",
      "now training  10.7 %\n",
      "71.67528093566838\n",
      "now training  10.8 %\n",
      "71.80736652922091\n",
      "now training  10.9 %\n",
      "71.24459676925706\n",
      "now training  11.0 %\n",
      "71.77685923376664\n",
      "now training  11.1 %\n",
      "71.80541895760089\n",
      "now training  11.2 %\n",
      "71.87191812073817\n",
      "now training  11.3 %\n",
      "71.7011305026006\n",
      "now training  11.4 %\n",
      "72.04884284397478\n",
      "now training  11.5 %\n",
      "72.12696876141982\n",
      "now training  11.6 %\n",
      "72.20441369724658\n",
      "now training  11.7 %\n",
      "72.17576845408175\n",
      "now training  11.8 %\n",
      "72.53385590408567\n",
      "now training  11.9 %\n",
      "72.53920714829856\n",
      "now training  12.0 %\n",
      "72.26803663108895\n",
      "now training  12.1 %\n",
      "72.32032240831767\n",
      "now training  12.2 %\n",
      "72.64922872185288\n",
      "now training  12.3 %\n",
      "72.48889915841438\n",
      "now training  12.4 %\n",
      "72.64970590582547\n",
      "now training  12.5 %\n",
      "72.67314313526938\n",
      "now training  12.6 %\n",
      "72.83658462793522\n",
      "now training  12.7 %\n",
      "72.47177804554357\n",
      "now training  12.8 %\n",
      "72.58304076087506\n",
      "now training  12.9 %\n",
      "72.7807084803969\n",
      "now training  13.0 %\n",
      "73.01766181379222\n",
      "now training  13.1 %\n",
      "73.15239355982503\n",
      "now training  13.2 %\n",
      "72.83022163125129\n",
      "now training  13.3 %\n",
      "73.00713666865263\n",
      "now training  13.4 %\n",
      "73.0273109551613\n",
      "now training  13.5 %\n",
      "73.13104553805172\n",
      "now training  13.6 %\n",
      "72.96924517824641\n",
      "now training  13.7 %\n",
      "73.30373643209843\n",
      "now training  13.8 %\n",
      "73.2500837281842\n",
      "now training  13.9 %\n",
      "73.08464925643254\n",
      "now training  14.0 %\n",
      "73.2773518866906\n",
      "now training  14.1 %\n",
      "73.0810751793974\n",
      "now training  14.2 %\n",
      "73.2050165450368\n",
      "now training  14.3 %\n",
      "73.3359647293189\n",
      "now training  14.4 %\n",
      "73.463842074587\n",
      "now training  14.5 %\n",
      "73.46247893222444\n",
      "now training  14.6 %\n",
      "73.03277138507589\n",
      "now training  14.7 %\n",
      "73.3298518091326\n",
      "now training  14.8 %\n",
      "73.3462489494775\n",
      "now training  14.9 %\n",
      "73.24061766185928\n",
      "now training  15.0 %\n",
      "73.56514331865509\n",
      "now training  15.1 %\n",
      "73.61846180843608\n",
      "now training  15.2 %\n",
      "73.44769111211247\n",
      "now training  15.3 %\n",
      "73.65868588558742\n",
      "now training  15.4 %\n",
      "73.50436314981243\n",
      "now training  15.5 %\n",
      "73.4063532100583\n",
      "now training  15.6 %\n",
      "73.5734293516861\n",
      "now training  15.7 %\n",
      "73.71605537017739\n",
      "now training  15.8 %\n",
      "73.66413206086078\n",
      "now training  15.9 %\n",
      "73.89844880528523\n",
      "now training  16.0 %\n",
      "73.6453229959543\n",
      "now training  16.1 %\n",
      "73.64348330451513\n",
      "now training  16.2 %\n",
      "73.8541490050342\n",
      "now training  16.3 %\n",
      "73.82921780130789\n",
      "now training  16.4 %\n",
      "73.79785265182802\n",
      "now training  16.5 %\n",
      "73.83371366743057\n",
      "now training  16.6 %\n",
      "74.040819844557\n",
      "now training  16.7 %\n",
      "73.84427115175562\n",
      "now training  16.8 %\n",
      "73.89785677053993\n",
      "now training  16.9 %\n",
      "73.98135779463391\n",
      "now training  17.0 %\n",
      "73.81780443091837\n",
      "now training  17.1 %\n",
      "74.0219673442926\n",
      "now training  17.2 %\n",
      "74.06304764252302\n",
      "now training  17.3 %\n",
      "73.99703957195801\n",
      "now training  17.4 %\n",
      "74.02601766180113\n",
      "now training  17.5 %\n",
      "74.13343127113407\n",
      "now training  17.6 %\n",
      "74.18812626655702\n",
      "now training  17.7 %\n",
      "73.96500527383313\n",
      "now training  17.8 %\n",
      "74.11499427136823\n",
      "now training  17.9 %\n",
      "73.98071888762065\n",
      "now training  18.0 %\n",
      "74.12426546872628\n",
      "now training  18.1 %\n",
      "74.20008817550784\n",
      "now training  18.2 %\n",
      "74.13270444928123\n",
      "now training  18.3 %\n",
      "74.195096111261\n",
      "now training  18.4 %\n",
      "74.17555344805363\n",
      "now training  18.5 %\n",
      "74.10703140905983\n",
      "now training  18.6 %\n",
      "74.1776289333491\n",
      "now training  18.7 %\n",
      "74.4081921081369\n",
      "now training  18.8 %\n",
      "74.3323284605831\n",
      "now training  18.9 %\n",
      "74.29412781210716\n",
      "now training  19.0 %\n",
      "74.40126384932313\n",
      "now training  19.1 %\n",
      "74.27391216039798\n",
      "now training  19.2 %\n",
      "74.23190563177283\n",
      "now training  19.3 %\n",
      "74.33731004281448\n",
      "now training  19.4 %\n",
      "74.51227335312842\n",
      "now training  19.5 %\n",
      "74.5084615019473\n",
      "now training  19.6 %\n",
      "74.43369171606092\n",
      "now training  19.7 %\n",
      "74.58575844522213\n",
      "now training  19.8 %\n",
      "74.40903372871067\n",
      "now training  19.9 %\n",
      "74.38061809586395\n",
      "now training  20.0 %\n",
      "74.3946982962558\n",
      "now training  20.1 %\n",
      "74.50397360252343\n",
      "now training  20.2 %\n",
      "74.44660599117371\n",
      "now training  20.3 %\n",
      "74.44361352556432\n",
      "now training  20.4 %\n",
      "74.434753588963\n",
      "now training  20.5 %\n",
      "74.48233009687091\n",
      "now training  20.6 %\n",
      "74.55673868150001\n",
      "now training  20.7 %\n",
      "74.5314919728578\n",
      "now training  20.8 %\n",
      "74.52105556406717\n",
      "now training  20.9 %\n",
      "74.49455621355318\n",
      "now training  21.0 %\n",
      "74.64728774868189\n",
      "now training  21.1 %\n",
      "74.57345228581046\n",
      "now training  21.2 %\n",
      "74.62840479512944\n",
      "now training  21.3 %\n",
      "74.71682182512875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now training  21.4 %\n",
      "74.56084152492252\n",
      "now training  21.5 %\n",
      "74.61403932908556\n",
      "now training  21.6 %\n",
      "74.553892497533\n",
      "now training  21.7 %\n",
      "74.65313928638801\n",
      "now training  21.8 %\n",
      "74.475973273188\n",
      "now training  21.9 %\n",
      "74.65538459878499\n",
      "now training  22.0 %\n",
      "74.86247477438665\n",
      "now training  22.1 %\n",
      "74.68992635480753\n",
      "now training  22.2 %\n",
      "74.77951696023389\n",
      "now training  22.3 %\n",
      "74.76386856232808\n",
      "now training  22.4 %\n",
      "74.82444309128788\n",
      "now training  22.5 %\n",
      "74.70955153035881\n",
      "now training  22.6 %\n",
      "74.80117076624146\n",
      "now training  22.7 %\n",
      "74.74709603751567\n",
      "now training  22.8 %\n",
      "74.6711961569734\n",
      "now training  22.9 %\n",
      "74.81590561155194\n",
      "now training  23.0 %\n",
      "74.97071236240062\n",
      "now training  23.1 %\n",
      "74.96015427244012\n",
      "now training  23.2 %\n",
      "74.94696529694718\n",
      "now training  23.3 %\n",
      "74.90692125540654\n",
      "now training  23.4 %\n",
      "74.95666978218394\n",
      "now training  23.5 %\n",
      "74.97843818056862\n",
      "now training  23.6 %\n",
      "75.06173001147435\n",
      "now training  23.7 %\n",
      "74.96762282979122\n",
      "now training  23.8 %\n",
      "75.01737162636462\n",
      "now training  23.9 %\n",
      "74.93106693953767\n",
      "now training  24.0 %\n",
      "75.02580668459105\n",
      "now training  24.1 %\n",
      "74.9760366064854\n",
      "now training  24.2 %\n",
      "75.04510421844789\n",
      "now training  24.3 %\n",
      "75.00276556357902\n",
      "now training  24.4 %\n",
      "75.08497964079763\n",
      "now training  24.5 %\n",
      "75.11314820608376\n",
      "now training  24.6 %\n",
      "75.0489493357371\n",
      "now training  24.7 %\n",
      "75.12054501518692\n",
      "now training  24.8 %\n",
      "75.05429874742649\n",
      "now training  24.9 %\n",
      "75.02928048163591\n",
      "now training  25.0 %\n",
      "75.17145493481671\n",
      "now training  25.1 %\n",
      "75.09012413579696\n",
      "now training  25.2 %\n",
      "75.26534023612017\n",
      "now training  25.3 %\n",
      "75.20901128942816\n",
      "now training  25.4 %\n",
      "75.15815340375408\n",
      "now training  25.5 %\n",
      "75.12998268769931\n",
      "now training  25.6 %\n",
      "75.1591060591044\n",
      "now training  25.7 %\n",
      "75.20634816121733\n",
      "now training  25.8 %\n",
      "75.19875953399877\n",
      "now training  25.9 %\n",
      "75.1787971818969\n",
      "now training  26.0 %\n",
      "75.11257804970195\n",
      "now training  26.1 %\n",
      "75.14203365476975\n",
      "now training  26.2 %\n",
      "75.22131575699511\n",
      "now training  26.3 %\n",
      "75.19566433216632\n",
      "now training  26.4 %\n",
      "75.27241059275451\n",
      "now training  26.5 %\n",
      "75.29960581303611\n",
      "now training  26.6 %\n",
      "75.21640275337553\n",
      "now training  26.7 %\n",
      "75.29514929144084\n",
      "now training  26.8 %\n",
      "75.14463527265686\n",
      "now training  26.9 %\n",
      "75.22693099276877\n",
      "now training  27.0 %\n",
      "75.27855804464015\n",
      "now training  27.1 %\n",
      "75.30579624174216\n",
      "now training  27.2 %\n",
      "75.24509205881176\n",
      "now training  27.3 %\n",
      "75.3255399731172\n",
      "now training  27.4 %\n",
      "75.3097325385322\n",
      "now training  27.5 %\n",
      "75.29093870982886\n",
      "now training  27.6 %\n",
      "75.3714225007023\n",
      "now training  27.7 %\n",
      "75.14851122048415\n",
      "now training  27.8 %\n",
      "75.30943808906179\n",
      "now training  27.9 %\n",
      "75.30834619823068\n",
      "now training  28.0 %\n",
      "75.36070476682882\n",
      "now training  28.1 %\n",
      "75.45299359270297\n",
      "now training  28.2 %\n",
      "75.46309674186837\n",
      "now training  28.3 %\n",
      "75.42250077095497\n",
      "now training  28.4 %\n",
      "75.40131254393516\n",
      "now training  28.5 %\n",
      "75.37841576269456\n",
      "now training  28.6 %\n",
      "75.30007821383417\n",
      "now training  28.7 %\n",
      "75.42430938186214\n",
      "now training  28.8 %\n",
      "75.46443011742204\n",
      "now training  28.9 %\n",
      "75.23199468994031\n",
      "now training  29.0 %\n",
      "75.41017683505757\n",
      "now training  29.1 %\n",
      "75.35364418656397\n",
      "now training  29.2 %\n",
      "75.36878690142319\n",
      "now training  29.3 %\n",
      "75.44508591384876\n",
      "now training  29.4 %\n",
      "75.39484875205542\n",
      "now training  29.5 %\n",
      "75.44022880635144\n",
      "now training  29.6 %\n",
      "75.4977465455457\n",
      "now training  29.7 %\n",
      "75.42400426320665\n",
      "now training  29.8 %\n",
      "75.3985089547477\n",
      "now training  29.9 %\n",
      "75.44989964653378\n",
      "now training  30.0 %\n",
      "75.37983656753654\n",
      "now training  30.1 %\n",
      "75.46735937952457\n",
      "now training  30.2 %\n",
      "75.49815426402657\n",
      "now training  30.3 %\n",
      "75.55455531211531\n",
      "now training  30.4 %\n",
      "75.53553580213809\n",
      "now training  30.5 %\n",
      "75.52296333067123\n",
      "now training  30.6 %\n",
      "75.51819258516029\n",
      "now training  30.7 %\n",
      "75.52892647204733\n",
      "now training  30.8 %\n",
      "75.5209395506313\n",
      "now training  30.9 %\n",
      "75.67552264987549\n",
      "now training  31.0 %\n",
      "75.56496702869106\n",
      "now training  31.1 %\n",
      "75.51571487345213\n",
      "now training  31.2 %\n",
      "75.4624895660992\n",
      "now training  31.3 %\n",
      "75.5300039785594\n",
      "now training  31.4 %\n",
      "75.50150160940835\n",
      "now training  31.5 %\n",
      "75.51136089257463\n",
      "now training  31.6 %\n",
      "75.523789838396\n",
      "now training  31.7 %\n",
      "75.53095144719306\n",
      "now training  31.8 %\n",
      "75.48308474377775\n",
      "now training  31.9 %\n",
      "75.58780918343503\n",
      "now training  32.0 %\n",
      "75.54023993309269\n",
      "now training  32.1 %\n",
      "75.6504874758756\n",
      "now training  32.2 %\n",
      "75.64474137459534\n",
      "now training  32.3 %\n",
      "75.65971062563831\n",
      "now training  32.4 %\n",
      "75.76708214670809\n",
      "now training  32.5 %\n",
      "75.56283770177436\n",
      "now training  32.6 %\n",
      "75.64272049974039\n",
      "now training  32.7 %\n",
      "75.56629464769249\n",
      "now training  32.8 %\n",
      "75.61590295018111\n",
      "now training  32.9 %\n",
      "75.57985820575843\n",
      "now training  33.0 %\n",
      "75.6636318113135\n",
      "now training  33.1 %\n",
      "75.5659414595512\n",
      "now training  33.2 %\n",
      "75.66426794793932\n",
      "now training  33.3 %\n",
      "75.67913115427744\n",
      "now training  33.4 %\n",
      "75.69897034817592\n",
      "now training  33.5 %\n",
      "75.65233600459752\n",
      "now training  33.6 %\n",
      "75.67895541178606\n",
      "now training  33.7 %\n",
      "75.70142383196874\n",
      "now training  33.8 %\n",
      "75.6430096425827\n",
      "now training  33.9 %\n",
      "75.71853011581713\n",
      "now training  34.0 %\n",
      "75.77088685910336\n",
      "now training  34.1 %\n",
      "75.70470332439905\n",
      "now training  34.2 %\n",
      "75.69767623619289\n",
      "now training  34.3 %\n",
      "75.77492259101432\n",
      "now training  34.4 %\n",
      "75.67695180933379\n",
      "now training  34.5 %\n",
      "75.61900814078936\n",
      "now training  34.6 %\n",
      "75.66244685300774\n",
      "now training  34.7 %\n",
      "75.77518324909751\n",
      "now training  34.8 %\n",
      "75.7137075335101\n",
      "now training  34.9 %\n",
      "75.71074200967958\n",
      "now training  35.0 %\n",
      "75.70184740133965\n",
      "now training  35.1 %\n",
      "75.72302442587377\n",
      "now training  35.2 %\n",
      "75.75417146929212\n",
      "now training  35.3 %\n",
      "75.82289245506519\n",
      "now training  35.4 %\n",
      "75.88040161875614\n",
      "now training  35.5 %\n",
      "75.79012319520834\n",
      "now training  35.6 %\n",
      "75.79242013627764\n",
      "now training  35.7 %\n",
      "75.8201203770418\n",
      "now training  35.8 %\n",
      "75.82614752044005\n",
      "now training  35.9 %\n",
      "75.81373605663029\n",
      "now training  36.0 %\n",
      "75.80640219148383\n",
      "now training  36.1 %\n",
      "75.73424645631435\n",
      "now training  36.2 %\n",
      "75.84094464511692\n",
      "now training  36.3 %\n",
      "75.84858895207115\n",
      "now training  36.4 %\n",
      "75.8029172347307\n",
      "now training  36.5 %\n",
      "75.81491106976151\n",
      "now training  36.6 %\n",
      "75.81626024082705\n",
      "now training  36.7 %\n",
      "75.91819394064603\n",
      "now training  36.8 %\n",
      "75.97867492682165\n",
      "now training  36.9 %\n",
      "75.87704131275632\n",
      "now training  37.0 %\n",
      "75.88557074221197\n",
      "now training  37.1 %\n",
      "75.89630837596589\n",
      "now training  37.2 %\n",
      "75.84913726319206\n",
      "now training  37.3 %\n",
      "75.87529111402641\n",
      "now training  37.4 %\n",
      "75.88085177476118\n",
      "now training  37.5 %\n",
      "75.8911639355792\n",
      "now training  37.6 %\n",
      "75.93354615422436\n",
      "now training  37.7 %\n",
      "75.97004892695254\n",
      "now training  37.8 %\n",
      "75.94558287613111\n",
      "now training  37.9 %\n",
      "75.92614754542976\n",
      "now training  38.0 %\n",
      "75.97270884559074\n",
      "now training  38.1 %\n",
      "76.05294409830594\n",
      "now training  38.2 %\n",
      "75.97848980641706\n",
      "now training  38.3 %\n",
      "75.98007679906824\n",
      "now training  38.4 %\n",
      "75.99858918004105\n",
      "now training  38.5 %\n",
      "75.89551252197549\n",
      "now training  38.6 %\n",
      "75.95684252927153\n",
      "now training  38.7 %\n",
      "75.97309178020996\n",
      "now training  38.8 %\n",
      "76.01619099588419\n",
      "now training  38.9 %\n",
      "75.99613649785856\n",
      "now training  39.0 %\n",
      "76.06141820750861\n",
      "now training  39.1 %\n",
      "76.01168977930605\n",
      "now training  39.2 %\n",
      "76.08228096484909\n",
      "now training  39.3 %\n",
      "76.0304442410147\n",
      "now training  39.4 %\n",
      "75.99730431635054\n",
      "now training  39.5 %\n",
      "76.05055866576909\n",
      "now training  39.6 %\n",
      "75.93677841250125\n",
      "now training  39.7 %\n",
      "76.03015920912605\n",
      "now training  39.8 %\n",
      "75.8974185573414\n",
      "now training  39.9 %\n",
      "76.02859318780953\n",
      "now training  40.0 %\n",
      "76.01266830265706\n",
      "now training  40.1 %\n",
      "76.0580744364321\n",
      "now training  40.2 %\n",
      "76.04123428111515\n",
      "now training  40.3 %\n",
      "76.11197874828527\n",
      "now training  40.4 %\n",
      "76.09681568937509\n",
      "now training  40.5 %\n",
      "76.11902374580424\n",
      "now training  40.6 %\n",
      "76.1413258279066\n",
      "now training  40.7 %\n",
      "76.03713192146081\n",
      "now training  40.8 %\n",
      "76.16574077807783\n",
      "now training  40.9 %\n",
      "76.08135327267803\n",
      "now training  41.0 %\n",
      "76.11419588988164\n",
      "now training  41.1 %\n",
      "76.11829859405984\n",
      "now training  41.2 %\n",
      "76.17786508787917\n",
      "now training  41.3 %\n",
      "76.11060870275021\n",
      "now training  41.4 %\n",
      "76.15509028708139\n",
      "now training  41.5 %\n",
      "76.12464346112783\n",
      "now training  41.6 %\n",
      "76.11555586260194\n",
      "now training  41.7 %\n",
      "76.06646128936839\n",
      "now training  41.8 %\n",
      "76.06179384517341\n",
      "now training  41.9 %\n",
      "76.19196423218399\n",
      "now training  42.0 %\n",
      "76.11558766865889\n",
      "now training  42.1 %\n",
      "76.10477416083378\n",
      "now training  42.2 %\n",
      "76.147205598658\n",
      "now training  42.3 %\n",
      "76.11890054414125\n",
      "now training  42.4 %\n",
      "76.14976539507363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now training  42.5 %\n",
      "76.142034128451\n",
      "now training  42.6 %\n",
      "76.18505053567708\n",
      "now training  42.7 %\n",
      "76.09305423662991\n",
      "now training  42.8 %\n",
      "76.17389428974609\n",
      "now training  42.9 %\n",
      "76.14780602416907\n",
      "now training  43.0 %\n",
      "76.22093446022251\n",
      "now training  43.1 %\n",
      "76.10991981516219\n",
      "now training  43.2 %\n",
      "76.13897550676462\n",
      "now training  43.3 %\n",
      "76.15580522423177\n",
      "now training  43.4 %\n",
      "76.21027380616674\n",
      "now training  43.5 %\n",
      "76.28931178204844\n",
      "now training  43.6 %\n",
      "76.24121252361827\n",
      "now training  43.7 %\n",
      "76.19239515607505\n",
      "now training  43.8 %\n",
      "76.26141848180593\n",
      "now training  43.9 %\n",
      "76.23139401462663\n",
      "now training  44.0 %\n",
      "76.17124866541023\n",
      "now training  44.1 %\n",
      "76.2883366795318\n",
      "now training  44.2 %\n",
      "76.27915697874396\n",
      "now training  44.3 %\n",
      "76.28624595343217\n",
      "now training  44.4 %\n",
      "76.24155483521326\n",
      "now training  44.5 %\n",
      "76.25716704662238\n",
      "now training  44.6 %\n",
      "76.30470645638621\n",
      "now training  44.7 %\n",
      "76.20225857699164\n",
      "now training  44.8 %\n",
      "76.27382027302178\n",
      "now training  44.9 %\n",
      "76.19850946136944\n",
      "now training  45.0 %\n",
      "76.27253666264825\n",
      "now training  45.1 %\n",
      "76.35628503502457\n",
      "now training  45.2 %\n",
      "76.23922787946375\n",
      "now training  45.3 %\n",
      "76.32779880615541\n",
      "now training  45.4 %\n",
      "76.26890734193402\n",
      "now training  45.5 %\n",
      "76.2356960449137\n",
      "now training  45.6 %\n",
      "76.31130122744914\n",
      "now training  45.7 %\n",
      "76.3094962368484\n",
      "now training  45.8 %\n",
      "76.28687156780197\n",
      "now training  45.9 %\n",
      "76.30678129114364\n",
      "now training  46.0 %\n",
      "76.33184022928769\n",
      "now training  46.1 %\n",
      "76.266068609479\n",
      "now training  46.2 %\n",
      "76.29374670285114\n",
      "now training  46.3 %\n",
      "76.32059922727073\n",
      "now training  46.4 %\n",
      "76.27735357889789\n",
      "now training  46.5 %\n",
      "76.33869844078495\n",
      "now training  46.6 %\n",
      "76.33268108377452\n",
      "now training  46.7 %\n",
      "76.23217480539488\n",
      "now training  46.8 %\n",
      "76.31492552258642\n",
      "now training  46.9 %\n",
      "76.23621544689914\n",
      "now training  47.0 %\n",
      "76.35342239561382\n",
      "now training  47.1 %\n",
      "76.32148215332019\n",
      "now training  47.2 %\n",
      "76.35449930046843\n",
      "now training  47.3 %\n",
      "76.33110812976709\n",
      "now training  47.4 %\n",
      "76.32654073814135\n",
      "now training  47.5 %\n",
      "76.37646523025391\n",
      "now training  47.6 %\n",
      "76.42361803266317\n",
      "now training  47.7 %\n",
      "76.3808057868559\n",
      "now training  47.8 %\n",
      "76.31779661097018\n",
      "now training  47.9 %\n",
      "76.29796441309374\n",
      "now training  48.0 %\n",
      "76.3917452146317\n",
      "now training  48.1 %\n",
      "76.33266772191301\n",
      "now training  48.2 %\n",
      "76.28614075400817\n",
      "now training  48.3 %\n",
      "76.35426297677228\n",
      "now training  48.4 %\n",
      "76.39088964532996\n",
      "now training  48.5 %\n",
      "76.38834143678677\n",
      "now training  48.6 %\n",
      "76.37757857097053\n",
      "now training  48.7 %\n",
      "76.43655604485487\n",
      "now training  48.8 %\n",
      "76.38515186844171\n",
      "now training  48.9 %\n",
      "76.39177980813189\n",
      "now training  49.0 %\n",
      "76.42887361979608\n",
      "now training  49.1 %\n",
      "76.36437217961316\n",
      "now training  49.2 %\n",
      "76.35573153387308\n",
      "now training  49.3 %\n",
      "76.39585716879718\n",
      "now training  49.4 %\n",
      "76.35575830021914\n",
      "now training  49.5 %\n",
      "76.42636736838045\n",
      "now training  49.6 %\n",
      "76.42913823268049\n",
      "now training  49.7 %\n",
      "76.40476189370904\n",
      "now training  49.8 %\n",
      "76.43079447928609\n",
      "now training  49.9 %\n",
      "76.40577453047305\n",
      "now training  50.0 %\n",
      "76.30585351142535\n",
      "now training  50.1 %\n",
      "76.45118472236551\n",
      "now training  50.2 %\n",
      "76.4537023122647\n",
      "now training  50.3 %\n",
      "76.44036868238736\n",
      "now training  50.4 %\n",
      "76.44629599649262\n",
      "now training  50.5 %\n",
      "76.4055369635345\n",
      "now training  50.6 %\n",
      "76.37952213848574\n",
      "now training  50.7 %\n",
      "76.39824992507357\n",
      "now training  50.8 %\n",
      "76.48610055291752\n",
      "now training  50.9 %\n",
      "76.4044839938836\n",
      "now training  51.0 %\n",
      "76.42091724964789\n",
      "now training  51.1 %\n",
      "76.37539102314281\n",
      "now training  51.2 %\n",
      "76.46336075820571\n",
      "now training  51.3 %\n",
      "76.48622300620016\n",
      "now training  51.4 %\n",
      "76.43529693529894\n",
      "now training  51.5 %\n",
      "76.4802735483011\n",
      "now training  51.6 %\n",
      "76.51206808968773\n",
      "now training  51.7 %\n",
      "76.41747565456922\n",
      "now training  51.8 %\n",
      "76.44723206665078\n",
      "now training  51.9 %\n",
      "76.39092674576916\n",
      "now training  52.0 %\n",
      "76.46655952194706\n",
      "now training  52.1 %\n",
      "76.51573313093284\n",
      "now training  52.2 %\n",
      "76.49804702278755\n",
      "now training  52.3 %\n",
      "76.46168385253705\n",
      "now training  52.4 %\n",
      "76.43305303671801\n",
      "now training  52.5 %\n",
      "76.4696939269945\n",
      "now training  52.6 %\n",
      "76.45703160722923\n",
      "now training  52.7 %\n",
      "76.45464755622136\n",
      "now training  52.8 %\n",
      "76.49536582767348\n",
      "now training  52.9 %\n",
      "76.50311310263822\n",
      "now training  53.0 %\n",
      "76.53958303278174\n",
      "now training  53.1 %\n",
      "76.47874056272806\n",
      "now training  53.2 %\n",
      "76.5061654429919\n",
      "now training  53.3 %\n",
      "76.53784872497975\n",
      "now training  53.4 %\n",
      "76.54340553617423\n",
      "now training  53.5 %\n",
      "76.44897798150583\n",
      "now training  53.6 %\n",
      "76.55947137553989\n",
      "now training  53.7 %\n",
      "76.46704407012085\n",
      "now training  53.8 %\n",
      "76.49520649728002\n",
      "now training  53.9 %\n",
      "76.40414548733047\n",
      "now training  54.0 %\n",
      "76.51326755919855\n",
      "now training  54.1 %\n",
      "76.51288583082378\n",
      "now training  54.2 %\n",
      "76.48275052680033\n",
      "now training  54.3 %\n",
      "76.54505243223389\n",
      "now training  54.4 %\n",
      "76.53763434551985\n",
      "now training  54.5 %\n",
      "76.47139126364362\n",
      "now training  54.6 %\n",
      "76.47372940974655\n",
      "now training  54.7 %\n",
      "76.50980848807907\n",
      "now training  54.8 %\n",
      "76.47428441173756\n",
      "now training  54.9 %\n",
      "76.52690259643455\n",
      "now training  55.0 %\n",
      "76.53560550008437\n",
      "now training  55.1 %\n",
      "76.55444185307238\n",
      "now training  55.2 %\n",
      "76.53433490376092\n",
      "now training  55.3 %\n",
      "76.49279291289587\n",
      "now training  55.4 %\n",
      "76.49006206359631\n",
      "now training  55.5 %\n",
      "76.51792110753654\n",
      "now training  55.6 %\n",
      "76.58054538471944\n",
      "now training  55.7 %\n",
      "76.55200483540001\n",
      "now training  55.8 %\n",
      "76.5038982269165\n",
      "now training  55.9 %\n",
      "76.52028358188468\n",
      "now training  56.0 %\n",
      "76.51794331284769\n",
      "now training  56.1 %\n",
      "76.55477763874698\n",
      "now training  56.2 %\n",
      "76.5027586505786\n",
      "now training  56.3 %\n",
      "76.5683353720135\n",
      "now training  56.4 %\n",
      "76.51034662518653\n",
      "now training  56.5 %\n",
      "76.55202828789778\n",
      "now training  56.6 %\n",
      "76.51067149644616\n",
      "now training  56.7 %\n",
      "76.61299692811181\n",
      "now training  56.8 %\n",
      "76.56366501670446\n",
      "now training  56.9 %\n",
      "76.60320437144472\n",
      "now training  57.0 %\n",
      "76.58671425172523\n",
      "now training  57.1 %\n",
      "76.62033100507938\n",
      "now training  57.2 %\n",
      "76.50712656830406\n",
      "now training  57.3 %\n",
      "76.60513329557752\n",
      "now training  57.4 %\n",
      "76.55138916069221\n",
      "now training  57.5 %\n",
      "76.58129333638725\n",
      "now training  57.6 %\n",
      "76.54249161563408\n",
      "now training  57.7 %\n",
      "76.6043192346276\n",
      "now training  57.8 %\n",
      "76.57572850081272\n",
      "now training  57.9 %\n",
      "76.60983448581483\n",
      "now training  58.0 %\n",
      "76.60761989964529\n",
      "now training  58.1 %\n",
      "76.6112030144864\n",
      "now training  58.2 %\n",
      "76.58443885059974\n",
      "now training  58.3 %\n",
      "76.63145016681725\n",
      "now training  58.4 %\n",
      "76.56394462643942\n",
      "now training  58.5 %\n",
      "76.56018266729187\n",
      "now training  58.6 %\n",
      "76.60181371646101\n",
      "now training  58.7 %\n",
      "76.63117716115262\n",
      "now training  58.8 %\n",
      "76.59355972570853\n",
      "now training  58.9 %\n",
      "76.65472031461847\n",
      "now training  59.0 %\n",
      "76.65385072341844\n",
      "now training  59.1 %\n",
      "76.66231024822861\n",
      "now training  59.2 %\n",
      "76.62212889888487\n",
      "now training  59.3 %\n",
      "76.60903575406768\n",
      "now training  59.4 %\n",
      "76.55359601370307\n",
      "now training  59.5 %\n",
      "76.62393068680107\n",
      "now training  59.6 %\n",
      "76.64547807354535\n",
      "now training  59.7 %\n",
      "76.58289579487534\n",
      "now training  59.8 %\n",
      "76.55855866304617\n",
      "now training  59.9 %\n",
      "76.62715711884542\n",
      "now training  60.0 %\n",
      "76.66545204317312\n",
      "now training  60.1 %\n",
      "76.60349993885646\n",
      "now training  60.2 %\n",
      "76.65749880468593\n",
      "now training  60.3 %\n",
      "76.61682791515884\n",
      "now training  60.4 %\n",
      "76.69976923338048\n",
      "now training  60.5 %\n",
      "76.58380059409276\n",
      "now training  60.6 %\n",
      "76.68988141334879\n",
      "now training  60.7 %\n",
      "76.71079448072426\n",
      "now training  60.8 %\n",
      "76.6316501868147\n",
      "now training  60.9 %\n",
      "76.67306396334567\n",
      "now training  61.0 %\n",
      "76.62544432748795\n",
      "now training  61.1 %\n",
      "76.65569207682525\n",
      "now training  61.2 %\n",
      "76.6808614094675\n",
      "now training  61.3 %\n",
      "76.611661124948\n",
      "now training  61.4 %\n",
      "76.63663986277271\n",
      "now training  61.5 %\n",
      "76.65454921962622\n",
      "now training  61.6 %\n",
      "76.70866642385205\n",
      "now training  61.7 %\n",
      "76.69176780188901\n",
      "now training  61.8 %\n",
      "76.65376391540644\n",
      "now training  61.9 %\n",
      "76.67044449235138\n",
      "now training  62.0 %\n",
      "76.65504729030107\n",
      "now training  62.1 %\n",
      "76.65347069696482\n",
      "now training  62.2 %\n",
      "76.70003772389039\n",
      "now training  62.3 %\n",
      "76.74495481666412\n",
      "now training  62.4 %\n",
      "76.72160754836831\n",
      "now training  62.5 %\n",
      "76.63621797402733\n",
      "now training  62.6 %\n",
      "76.66673262611556\n",
      "now training  62.7 %\n",
      "76.65760523732507\n",
      "now training  62.8 %\n",
      "76.67029870019556\n",
      "now training  62.9 %\n",
      "76.66659460402553\n",
      "now training  63.0 %\n",
      "76.6312058353954\n",
      "now training  63.1 %\n",
      "76.67721596676411\n",
      "now training  63.2 %\n",
      "76.67140781558201\n",
      "now training  63.3 %\n",
      "76.68235488657179\n",
      "now training  63.4 %\n",
      "76.6761387390765\n",
      "now training  63.5 %\n",
      "76.72517647432063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now training  63.6 %\n",
      "76.71302741417813\n",
      "now training  63.7 %\n",
      "76.70968468546452\n",
      "now training  63.8 %\n",
      "76.66589341250331\n",
      "now training  63.9 %\n",
      "76.71728700602212\n",
      "now training  64.0 %\n",
      "76.73462544083729\n",
      "now training  64.1 %\n",
      "76.727641394614\n",
      "now training  64.2 %\n",
      "76.7624295711649\n",
      "now training  64.3 %\n",
      "76.7413964433502\n",
      "now training  64.4 %\n",
      "76.73969001388042\n",
      "now training  64.5 %\n",
      "76.72823748722983\n",
      "now training  64.6 %\n",
      "76.70009902753668\n",
      "now training  64.7 %\n",
      "76.76115946887856\n",
      "now training  64.8 %\n",
      "76.69627368097507\n",
      "now training  64.9 %\n",
      "76.74777199815207\n",
      "now training  65.0 %\n",
      "76.73480334205519\n",
      "now training  65.1 %\n",
      "76.74330294290087\n",
      "now training  65.2 %\n",
      "76.70236592029927\n",
      "now training  65.3 %\n",
      "76.7238149458022\n",
      "now training  65.4 %\n",
      "76.73412711124509\n",
      "now training  65.5 %\n",
      "76.67781866291436\n",
      "now training  65.6 %\n",
      "76.78183740672058\n",
      "now training  65.7 %\n",
      "76.7499785280167\n",
      "now training  65.8 %\n",
      "76.7181211678361\n",
      "now training  65.9 %\n",
      "76.77721815777826\n",
      "now training  66.0 %\n",
      "76.81341596677177\n",
      "now training  66.1 %\n",
      "76.83148152417215\n",
      "now training  66.2 %\n",
      "76.81711800306489\n",
      "now training  66.3 %\n",
      "76.75440373926149\n",
      "now training  66.4 %\n",
      "76.78076799675507\n",
      "now training  66.5 %\n",
      "76.76244245188377\n",
      "now training  66.6 %\n",
      "76.78443459860469\n",
      "now training  66.7 %\n",
      "76.78766429661701\n",
      "now training  66.8 %\n",
      "76.79334780771435\n",
      "now training  66.9 %\n",
      "76.81371705368211\n",
      "now training  67.0 %\n",
      "76.8129123836718\n",
      "now training  67.1 %\n",
      "76.76979671813757\n",
      "now training  67.2 %\n",
      "76.74010165268876\n",
      "now training  67.3 %\n",
      "76.77062907954296\n",
      "now training  67.4 %\n",
      "76.78767968133823\n",
      "now training  67.5 %\n",
      "76.7285554331531\n",
      "now training  67.6 %\n",
      "76.7635355719764\n",
      "now training  67.7 %\n",
      "76.73054084569738\n",
      "now training  67.8 %\n",
      "76.77592624653354\n",
      "now training  67.9 %\n",
      "76.75037344479941\n",
      "now training  68.0 %\n",
      "76.70905091273967\n",
      "now training  68.1 %\n",
      "76.76486552432435\n",
      "now training  68.2 %\n",
      "76.77048125648284\n",
      "now training  68.3 %\n",
      "76.81915075406226\n",
      "now training  68.4 %\n",
      "76.82996903562398\n",
      "now training  68.5 %\n",
      "76.77839326149955\n",
      "now training  68.6 %\n",
      "76.87671748270317\n",
      "now training  68.7 %\n",
      "76.79530230888972\n",
      "now training  68.8 %\n",
      "76.78673454998896\n",
      "now training  68.9 %\n",
      "76.8282581330735\n",
      "now training  69.0 %\n",
      "76.89677906904085\n",
      "now training  69.1 %\n",
      "76.8493464032475\n",
      "now training  69.2 %\n",
      "76.80349664099518\n",
      "now training  69.3 %\n",
      "76.7902925046815\n",
      "now training  69.4 %\n",
      "76.79821374852747\n",
      "now training  69.5 %\n",
      "76.77031518877531\n",
      "now training  69.6 %\n",
      "76.8207406208232\n",
      "now training  69.7 %\n",
      "76.82923705955741\n",
      "now training  69.8 %\n",
      "76.84426502507814\n",
      "now training  69.9 %\n",
      "76.7941846639694\n",
      "now training  70.0 %\n",
      "76.81733287445446\n",
      "now training  70.1 %\n",
      "76.79976143221793\n",
      "now training  70.2 %\n",
      "76.79304371778495\n",
      "now training  70.3 %\n",
      "76.84450630739062\n",
      "now training  70.4 %\n",
      "76.78542986296686\n",
      "now training  70.5 %\n",
      "76.84852802185814\n",
      "now training  70.6 %\n",
      "76.79461948368895\n",
      "now training  70.7 %\n",
      "76.8529240334161\n",
      "now training  70.8 %\n",
      "76.78340440960748\n",
      "now training  70.9 %\n",
      "76.86119151872435\n",
      "now training  71.0 %\n",
      "76.79131192830054\n",
      "now training  71.1 %\n",
      "76.82775725563066\n",
      "now training  71.2 %\n",
      "76.86173067359859\n",
      "now training  71.3 %\n",
      "76.8404241315366\n",
      "now training  71.4 %\n",
      "76.8389122101559\n",
      "now training  71.5 %\n",
      "76.84732507204086\n",
      "now training  71.6 %\n",
      "76.81267380544281\n",
      "now training  71.7 %\n",
      "76.8629729751226\n",
      "now training  71.8 %\n",
      "76.793656852214\n",
      "now training  71.9 %\n",
      "76.80890873034558\n",
      "now training  72.0 %\n",
      "76.85411416450752\n",
      "now training  72.1 %\n",
      "76.75620866330348\n",
      "now training  72.2 %\n",
      "76.87113956606974\n",
      "now training  72.3 %\n",
      "76.86950302732917\n",
      "now training  72.4 %\n",
      "76.82641500866038\n",
      "now training  72.5 %\n",
      "76.82456029073091\n",
      "now training  72.6 %\n",
      "76.86379720136213\n",
      "now training  72.7 %\n",
      "76.88216668606742\n",
      "now training  72.8 %\n",
      "76.87813094077165\n",
      "now training  72.9 %\n",
      "76.87603343187469\n",
      "now training  73.0 %\n",
      "76.91968812242877\n",
      "now training  73.1 %\n",
      "76.8792272429237\n",
      "now training  73.2 %\n",
      "76.92884460987223\n",
      "now training  73.3 %\n",
      "76.87202504028137\n",
      "now training  73.4 %\n",
      "76.87028089460658\n",
      "now training  73.5 %\n",
      "76.85924673429878\n",
      "now training  73.6 %\n",
      "76.84378725591122\n",
      "now training  73.7 %\n",
      "76.8994016355413\n",
      "now training  73.8 %\n",
      "76.84066059465617\n",
      "now training  73.9 %\n",
      "76.83409495992066\n",
      "now training  74.0 %\n",
      "76.91337657799104\n",
      "now training  74.1 %\n",
      "76.90830210219671\n",
      "now training  74.2 %\n",
      "76.91275242874745\n",
      "now training  74.3 %\n",
      "76.92483518927123\n",
      "now training  74.4 %\n",
      "76.83712847480656\n",
      "now training  74.5 %\n",
      "76.93122818720676\n",
      "now training  74.6 %\n",
      "76.87790306193541\n",
      "now training  74.7 %\n",
      "76.89137802988138\n",
      "now training  74.8 %\n",
      "76.85835481951985\n",
      "now training  74.9 %\n",
      "76.89990358504393\n",
      "now training  75.0 %\n",
      "76.94042571825332\n",
      "now training  75.1 %\n",
      "76.85533698566415\n",
      "now training  75.2 %\n",
      "76.97606995593905\n",
      "now training  75.3 %\n",
      "76.88904754658387\n",
      "now training  75.4 %\n",
      "76.90834178120377\n",
      "now training  75.5 %\n",
      "76.8796283305845\n",
      "now training  75.6 %\n",
      "76.93073703063055\n",
      "now training  75.7 %\n",
      "76.97269996702566\n",
      "now training  75.8 %\n",
      "76.89836844827013\n",
      "now training  75.9 %\n",
      "76.92779618332625\n",
      "now training  76.0 %\n",
      "76.89705767175951\n",
      "now training  76.1 %\n",
      "76.93793553913318\n",
      "now training  76.2 %\n",
      "76.90489205022863\n",
      "now training  76.3 %\n",
      "76.94833967715039\n",
      "now training  76.4 %\n",
      "76.96986740834458\n",
      "now training  76.5 %\n",
      "76.9021330468519\n",
      "now training  76.6 %\n",
      "76.93758890427985\n",
      "now training  76.7 %\n",
      "76.92826406461553\n",
      "now training  76.8 %\n",
      "76.9456413518517\n",
      "now training  76.9 %\n",
      "76.91694607010011\n",
      "now training  77.0 %\n",
      "76.95907821251956\n",
      "now training  77.1 %\n",
      "76.93128531984144\n",
      "now training  77.2 %\n",
      "76.93193934579128\n",
      "now training  77.3 %\n",
      "76.9599059014803\n",
      "now training  77.4 %\n",
      "76.92599940082478\n",
      "now training  77.5 %\n",
      "76.96781706057861\n",
      "now training  77.6 %\n",
      "76.92406059270078\n",
      "now training  77.7 %\n",
      "76.94478869584283\n",
      "now training  77.8 %\n",
      "76.97392378934686\n",
      "now training  77.9 %\n",
      "76.94656587707445\n",
      "now training  78.0 %\n",
      "76.92499309975652\n",
      "now training  78.1 %\n",
      "76.90830601089543\n",
      "now training  78.2 %\n",
      "76.94457875557036\n",
      "now training  78.3 %\n",
      "76.9610624336987\n",
      "now training  78.4 %\n",
      "76.95977569643482\n",
      "now training  78.5 %\n",
      "76.97204916276806\n",
      "now training  78.6 %\n",
      "76.96208287254804\n",
      "now training  78.7 %\n",
      "76.9853128477918\n",
      "now training  78.8 %\n",
      "77.01114849810155\n",
      "now training  78.9 %\n",
      "76.95784695190922\n",
      "now training  79.0 %\n",
      "76.97010306155728\n",
      "now training  79.1 %\n",
      "76.93567515534212\n",
      "now training  79.2 %\n",
      "76.98423899304913\n",
      "now training  79.3 %\n",
      "76.97961194787544\n",
      "now training  79.4 %\n",
      "76.94738657407773\n",
      "now training  79.5 %\n",
      "76.93643531512038\n",
      "now training  79.6 %\n",
      "76.97910089948587\n",
      "now training  79.7 %\n",
      "76.90797757308074\n",
      "now training  79.8 %\n",
      "76.94058199952944\n",
      "now training  79.9 %\n",
      "76.99309545714233\n",
      "now training  80.0 %\n",
      "76.95195336800725\n",
      "now training  80.1 %\n",
      "76.9697022495436\n",
      "now training  80.2 %\n",
      "76.97656741085702\n",
      "now training  80.3 %\n",
      "76.95007335508744\n",
      "now training  80.4 %\n",
      "76.95624185371983\n",
      "now training  80.5 %\n",
      "77.0148243296194\n",
      "now training  80.6 %\n",
      "77.0005139558945\n",
      "now training  80.7 %\n",
      "76.93735535685049\n",
      "now training  80.8 %\n",
      "76.9503662197564\n",
      "now training  80.9 %\n",
      "76.95126729465639\n",
      "now training  81.0 %\n",
      "76.95974735971737\n",
      "now training  81.1 %\n",
      "76.96326892715032\n",
      "now training  81.2 %\n",
      "76.92883442528382\n",
      "now training  81.3 %\n",
      "76.98485693600692\n",
      "now training  81.4 %\n",
      "76.98529058103442\n",
      "now training  81.5 %\n",
      "76.9441367200803\n",
      "now training  81.6 %\n",
      "76.98701602998621\n",
      "now training  81.7 %\n",
      "76.9774391578329\n",
      "now training  81.8 %\n",
      "76.99607667860712\n",
      "now training  81.9 %\n",
      "76.96501331180562\n",
      "now training  82.0 %\n",
      "76.97280615047072\n",
      "now training  82.1 %\n",
      "76.99153709911982\n",
      "now training  82.2 %\n",
      "76.9883131422998\n",
      "now training  82.3 %\n",
      "76.98803689565732\n",
      "now training  82.4 %\n",
      "77.03514108302323\n",
      "now training  82.5 %\n",
      "76.99140688819774\n",
      "now training  82.6 %\n",
      "77.00614545743886\n",
      "now training  82.7 %\n",
      "76.9977075679344\n",
      "now training  82.8 %\n",
      "76.99383440043165\n",
      "now training  82.9 %\n",
      "76.97908502394033\n",
      "now training  83.0 %\n",
      "77.00172392894692\n",
      "now training  83.1 %\n",
      "76.96704106984495\n",
      "now training  83.2 %\n",
      "76.99093024589806\n",
      "now training  83.3 %\n",
      "76.92775113001358\n",
      "now training  83.4 %\n",
      "76.96916335113484\n",
      "now training  83.5 %\n",
      "76.98790000055058\n",
      "now training  83.6 %\n",
      "77.00687489120286\n",
      "now training  83.7 %\n",
      "77.04548395116383\n",
      "now training  83.8 %\n",
      "77.04343184422501\n",
      "now training  83.9 %\n",
      "76.97342145472182\n",
      "now training  84.0 %\n",
      "77.01841383421761\n",
      "now training  84.1 %\n",
      "76.99990748125974\n",
      "now training  84.2 %\n",
      "76.94530204897106\n",
      "now training  84.3 %\n",
      "77.05396337672163\n",
      "now training  84.4 %\n",
      "76.99989190695159\n",
      "now training  84.5 %\n",
      "77.01891975867088\n",
      "now training  84.6 %\n",
      "77.00052826462557\n",
      "now training  84.7 %\n",
      "77.02377376328623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now training  84.8 %\n",
      "77.0247222567735\n",
      "now training  84.9 %\n",
      "77.03191774198008\n",
      "now training  85.0 %\n",
      "77.03906802821318\n",
      "now training  85.1 %\n",
      "77.00059046464673\n",
      "now training  85.2 %\n",
      "77.00229891797868\n",
      "now training  85.3 %\n",
      "77.02646241585292\n",
      "now training  85.4 %\n",
      "77.04611276125732\n",
      "now training  85.5 %\n",
      "77.00155847988097\n",
      "now training  85.6 %\n",
      "77.0422356859774\n",
      "now training  85.7 %\n",
      "77.02292794022095\n",
      "now training  85.8 %\n",
      "77.03188215759215\n",
      "now training  85.9 %\n",
      "77.03031070467136\n",
      "now training  86.0 %\n",
      "77.00863597736338\n",
      "now training  86.1 %\n",
      "76.99434216132269\n",
      "now training  86.2 %\n",
      "76.99925929785266\n",
      "now training  86.3 %\n",
      "77.03237874292964\n",
      "now training  86.4 %\n",
      "77.01090441724803\n",
      "now training  86.5 %\n",
      "77.02452639593837\n",
      "now training  86.6 %\n",
      "77.06552928458211\n",
      "now training  86.7 %\n",
      "77.02596330253215\n",
      "now training  86.8 %\n",
      "77.04953496377344\n",
      "now training  86.9 %\n",
      "77.06305217976391\n",
      "now training  87.0 %\n",
      "77.02595767130965\n",
      "now training  87.1 %\n",
      "77.03696804119117\n",
      "now training  87.2 %\n",
      "77.02541228582326\n",
      "now training  87.3 %\n",
      "77.05142438780126\n",
      "now training  87.4 %\n",
      "77.07310877292583\n",
      "now training  87.5 %\n",
      "77.03792018141746\n",
      "now training  87.6 %\n",
      "77.07026185514317\n",
      "now training  87.7 %\n",
      "76.99906427077384\n",
      "now training  87.8 %\n",
      "77.06185075873724\n",
      "now training  87.9 %\n",
      "77.07954922124614\n",
      "now training  88.0 %\n",
      "77.0350244793552\n",
      "now training  88.1 %\n",
      "77.05139200498081\n",
      "now training  88.2 %\n",
      "77.05677717543279\n",
      "now training  88.3 %\n",
      "77.05333810317119\n",
      "now training  88.4 %\n",
      "77.0526460502805\n",
      "now training  88.5 %\n",
      "77.03464586207188\n",
      "now training  88.6 %\n",
      "77.03334888488452\n",
      "now training  88.7 %\n",
      "77.07670640455511\n",
      "now training  88.8 %\n",
      "77.04546415916984\n",
      "now training  88.9 %\n",
      "76.95723491294521\n",
      "now training  89.0 %\n",
      "77.06019565208135\n",
      "now training  89.1 %\n",
      "77.06902054791155\n",
      "now training  89.2 %\n",
      "77.03898950886014\n",
      "now training  89.3 %\n",
      "77.1021155791918\n",
      "now training  89.4 %\n",
      "77.04739946536421\n",
      "now training  89.5 %\n",
      "77.04656985708603\n",
      "now training  89.6 %\n",
      "77.08178980394493\n",
      "now training  89.7 %\n",
      "77.04380306027679\n",
      "now training  89.8 %\n",
      "77.05743544308591\n",
      "now training  89.9 %\n",
      "77.07875003970081\n",
      "now training  90.0 %\n",
      "77.11656573909899\n",
      "now training  90.1 %\n",
      "77.07268955420183\n",
      "now training  90.2 %\n",
      "77.05948476410046\n",
      "now training  90.3 %\n",
      "77.06321615517102\n",
      "now training  90.4 %\n",
      "77.07235952093981\n",
      "now training  90.5 %\n",
      "76.99977614488054\n",
      "now training  90.6 %\n",
      "77.01863428698458\n",
      "now training  90.7 %\n",
      "77.01887956544377\n",
      "now training  90.8 %\n",
      "77.08903292512592\n",
      "now training  90.9 %\n",
      "77.05415221712684\n",
      "now training  91.0 %\n",
      "77.08029448004615\n",
      "now training  91.1 %\n",
      "77.0282343115431\n",
      "now training  91.2 %\n",
      "77.07104307191504\n",
      "now training  91.3 %\n",
      "77.08448743779358\n",
      "now training  91.4 %\n",
      "77.08129652058007\n",
      "now training  91.5 %\n",
      "77.07655733601133\n",
      "now training  91.6 %\n",
      "77.09023109269933\n",
      "now training  91.7 %\n",
      "77.06688983339639\n",
      "now training  91.8 %\n",
      "77.11179068719235\n",
      "now training  91.9 %\n",
      "77.06086722927293\n",
      "now training  92.0 %\n",
      "77.10778154390316\n",
      "now training  92.1 %\n",
      "77.0692296833882\n",
      "now training  92.2 %\n",
      "77.08195960896158\n",
      "now training  92.3 %\n",
      "77.08799645983845\n",
      "now training  92.4 %\n",
      "77.10587633150952\n",
      "now training  92.5 %\n",
      "77.10532281997118\n",
      "now training  92.6 %\n",
      "77.04633858571287\n",
      "now training  92.7 %\n",
      "77.06512535701003\n",
      "now training  92.8 %\n",
      "77.11646469775366\n",
      "now training  92.9 %\n",
      "77.08873907204985\n",
      "now training  93.0 %\n",
      "77.07170317017656\n",
      "now training  93.1 %\n",
      "77.09323309721064\n",
      "now training  93.2 %\n",
      "77.07585081877727\n",
      "now training  93.3 %\n",
      "77.07260947906748\n",
      "now training  93.4 %\n",
      "77.07931788832335\n",
      "now training  93.5 %\n",
      "77.12352672120717\n",
      "now training  93.6 %\n",
      "77.03569150412731\n",
      "now training  93.7 %\n",
      "77.09055369776057\n",
      "now training  93.8 %\n",
      "77.12640172391404\n",
      "now training  93.9 %\n",
      "77.10594042811742\n",
      "now training  94.0 %\n",
      "77.09061588902367\n",
      "now training  94.1 %\n",
      "77.11627999963135\n",
      "now training  94.2 %\n",
      "77.13545775327711\n",
      "now training  94.3 %\n",
      "77.06884451995899\n",
      "now training  94.4 %\n",
      "77.07188237814853\n",
      "now training  94.5 %\n",
      "77.11826657920989\n",
      "now training  94.6 %\n",
      "77.13093342610239\n",
      "now training  94.7 %\n",
      "77.1155350690084\n",
      "now training  94.8 %\n",
      "77.12265911334404\n",
      "now training  94.9 %\n",
      "77.07845402677036\n",
      "now training  95.0 %\n",
      "77.12699131975305\n",
      "now training  95.1 %\n",
      "77.08864204177274\n",
      "now training  95.2 %\n",
      "77.11849051100016\n",
      "now training  95.3 %\n",
      "77.0810983601692\n",
      "now training  95.4 %\n",
      "77.14843704595502\n",
      "now training  95.5 %\n",
      "77.12562815062489\n",
      "now training  95.6 %\n",
      "77.11010567149648\n",
      "now training  95.7 %\n",
      "77.09561684294376\n",
      "now training  95.8 %\n",
      "77.10248918713626\n",
      "now training  95.9 %\n",
      "77.11874516594806\n",
      "now training  96.0 %\n",
      "77.14139591894173\n",
      "now training  96.1 %\n",
      "77.09985191326516\n",
      "now training  96.2 %\n",
      "77.1075458446811\n",
      "now training  96.3 %\n",
      "77.13178188053239\n",
      "now training  96.4 %\n",
      "77.13054604614821\n",
      "now training  96.5 %\n",
      "77.16076722519838\n",
      "now training  96.6 %\n",
      "77.13112767508404\n",
      "now training  96.7 %\n",
      "77.12420978185277\n",
      "now training  96.8 %\n",
      "77.15909194387595\n",
      "now training  96.9 %\n",
      "77.11652801797233\n",
      "now training  97.0 %\n",
      "77.13332447900967\n",
      "now training  97.1 %\n",
      "77.11464017044818\n",
      "now training  97.2 %\n",
      "77.10574053424295\n",
      "now training  97.3 %\n",
      "77.13527062021633\n",
      "now training  97.4 %\n",
      "77.17482618305698\n",
      "now training  97.5 %\n",
      "77.14547893334763\n",
      "now training  97.6 %\n",
      "77.16429884277994\n",
      "now training  97.7 %\n",
      "77.16913695462372\n",
      "now training  97.8 %\n",
      "77.1185808864086\n",
      "now training  97.9 %\n",
      "77.14859499662973\n",
      "now training  98.0 %\n",
      "77.14951293110197\n",
      "now training  98.1 %\n",
      "77.15350136823464\n",
      "now training  98.2 %\n",
      "77.1782136041911\n",
      "now training  98.3 %\n",
      "77.14793191113137\n",
      "now training  98.4 %\n",
      "77.14072893281747\n",
      "now training  98.5 %\n",
      "77.17987165417158\n",
      "now training  98.6 %\n",
      "77.12815706043854\n",
      "now training  98.7 %\n",
      "77.18128340574467\n",
      "now training  98.8 %\n",
      "77.14678162940339\n",
      "now training  98.9 %\n",
      "77.1591302636756\n",
      "now training  99.0 %\n",
      "77.19883988412514\n",
      "now training  99.1 %\n",
      "77.13328460871328\n",
      "now training  99.2 %\n",
      "77.15348853140405\n",
      "now training  99.3 %\n",
      "77.15972761136871\n",
      "now training  99.4 %\n",
      "77.18036244286166\n",
      "now training  99.5 %\n",
      "77.15927555574302\n",
      "now training  99.6 %\n",
      "77.18256646982043\n",
      "now training  99.7 %\n",
      "77.12750704606144\n",
      "now training  99.8 %\n",
      "77.17055790467585\n",
      "now training  99.9 %\n",
      "77.19721167274257\n"
     ]
    }
   ],
   "source": [
    "epochtimes = 200000\n",
    "mini_batch = 8\n",
    "bestloss1 = 1e30 #Mini Batch Loss\n",
    "preloss = 1e30   #decay the learning rate\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(epochtimes):\n",
    "    X_train_random = []\n",
    "    Y_train_random = []\n",
    "    for i in range(mini_batch):\n",
    "        rindex = random.randint(0,len(X_train)-1)\n",
    "        X_train_random.append(X_train[rindex,:])\n",
    "        Y_train_random.append(Y_train[rindex,:])\n",
    "    X_train_random = np.array(X_train_random)\n",
    "    Y_train_random = np.array(Y_train_random)\n",
    "    \n",
    "        \n",
    "    input_hidden = np.dot(X_train_random, weight_hidden) \n",
    "    output_hidden = ReLU(input_hidden)\n",
    "    \n",
    "    input_hidden2 = np.dot(output_hidden, weight_hidden2) \n",
    "    output_hidden2 = ReLU(input_hidden2)\n",
    "    input_hidden3 = np.dot(output_hidden2, weight_hidden3) \n",
    "    output_hidden3 = ReLU(input_hidden3)\n",
    "    input_hidden4 = np.dot(output_hidden3, weight_hidden4) \n",
    "    output_hidden4 = ReLU(input_hidden4)\n",
    "    input_hidden5 = np.dot(output_hidden4, weight_hidden5) \n",
    "    output_hidden5 = ReLU(input_hidden5)\n",
    "    input_op = np.dot(output_hidden5, weight_output)\n",
    "    output_op = ReLU(input_op)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #==========================================\n",
    "    \n",
    "    derror_douto = output_op - Y_train_random\n",
    "    douto_dino = ReLU_der(input_op)\n",
    "    dino_dwo = output_hidden5      \n",
    "    derror_dwo = np.dot(dino_dwo.T, derror_douto * douto_dino)\n",
    "    \n",
    "    #==========================================\n",
    "    \n",
    "    derror_dino = derror_douto * douto_dino\n",
    "    dino_douth5 = weight_output\n",
    "    derror_douth5 = np.dot(derror_dino, dino_douth5.T)\n",
    "    douth5_dinh5 = ReLU_der(input_hidden5)\n",
    "    dinh5_dwh5 = output_hidden4\n",
    "    derror_dwh5 = np.dot(dinh5_dwh5.T, douth5_dinh5 * derror_douth5)\n",
    "    \n",
    "    #==========================================\n",
    "    \n",
    "    derror_dinh5 = derror_douth5 * douth5_dinh5\n",
    "    dinh5_douth4 = weight_hidden5\n",
    "    derror_douth4 = np.dot(derror_dinh5, dinh5_douth4.T)\n",
    "    douth4_dinh4 = ReLU_der(input_hidden4)\n",
    "    dinh4_dwh4 = output_hidden3\n",
    "    derror_dwh4 = np.dot(dinh4_dwh4.T, douth4_dinh4 * derror_douth4)\n",
    "    \n",
    "    #==========================================\n",
    "    \n",
    "    derror_dinh4 = derror_douth4 * douth4_dinh4\n",
    "    dinh4_douth3 = weight_hidden4\n",
    "    derror_douth3 = np.dot(derror_dinh4, dinh4_douth3.T)\n",
    "    douth3_dinh3 = ReLU_der(input_hidden3)\n",
    "    dinh3_dwh3 = output_hidden2\n",
    "    derror_dwh3 = np.dot(dinh3_dwh3.T, douth3_dinh3 * derror_douth3)\n",
    "    \n",
    "    #==========================================\n",
    "    \n",
    "    derror_dinh3 = derror_douth3 * douth3_dinh3\n",
    "    dinh3_douth2 = weight_hidden3\n",
    "    derror_douth2 = np.dot(derror_dinh3, dinh3_douth2.T)\n",
    "    douth2_dinh2 = ReLU_der(input_hidden2)\n",
    "    dinh2_dwh2 = output_hidden\n",
    "    derror_dwh2 = np.dot(dinh2_dwh2.T, douth2_dinh2 * derror_douth2)\n",
    "    \n",
    "    #==========================================\n",
    "    \n",
    "    derror_dinh2 = derror_douth2 * douth2_dinh2\n",
    "    dinh2_douth = weight_hidden2\n",
    "    derror_douth = np.dot(derror_dinh2, dinh2_douth.T)\n",
    "    douth_dinh = ReLU_der(input_hidden)\n",
    "    dinh_dwh = X_train_random\n",
    "    derror_dwh = np.dot(dinh_dwh.T, douth_dinh * derror_douth)\n",
    "    \n",
    "    #==========================================\n",
    "       \n",
    "    \n",
    "    \n",
    "    weight_hidden = weight_hidden - limit(lr * derror_dwh)\n",
    "    weight_hidden2 = weight_hidden2 - limit(lr * derror_dwh2)\n",
    "    weight_hidden3 = weight_hidden3 - limit(lr * derror_dwh3)\n",
    "    weight_hidden4 = weight_hidden4 - limit(lr * derror_dwh4)\n",
    "    weight_hidden5 = weight_hidden5 - limit(lr * derror_dwh5)\n",
    "    weight_output = weight_output - limit(lr * derror_dwo)\n",
    "    \n",
    "    \n",
    "    error_out = ((1 / 2) * (np.power((output_op - Y_train_random), 2)))\n",
    "    #print(error_out.sum())\n",
    "    \n",
    "        \n",
    "        \n",
    "    #calulate error \n",
    "    if epoch % 200 == 0 : \n",
    "        input_hidden = np.dot(X_train, weight_hidden) \n",
    "        output_hidden = ReLU(input_hidden)\n",
    "        input_hidden2 = np.dot(input_hidden, weight_hidden2) \n",
    "        output_hidden2 = ReLU(input_hidden2)\n",
    "        input_hidden3 = np.dot(output_hidden2, weight_hidden3) \n",
    "        output_hidden3 = ReLU(input_hidden3)\n",
    "        input_hidden4 = np.dot(output_hidden3, weight_hidden4) \n",
    "        output_hidden4 = ReLU(input_hidden4)\n",
    "        input_hidden5 = np.dot(output_hidden4, weight_hidden5) \n",
    "        output_hidden5 = ReLU(input_hidden5)\n",
    "        input_op = np.dot(output_hidden5, weight_output)\n",
    "        output_op = ReLU(input_op)\n",
    "        error_out = ((1 / 2) * (np.power((output_op - Y_train), 2)))\n",
    "        \n",
    "        if epoch == 0 : \n",
    "            preloss = error_out.sum()     \n",
    "            \n",
    "        if error_out.sum() < preloss :\n",
    "            mbwh1 = np.copy(weight_hidden)     #collect the best minibatch weight vector\n",
    "            mbwh2 = np.copy(weight_hidden2)\n",
    "            mbwh3 = np.copy(weight_hidden3)\n",
    "            mbwh4 = np.copy(weight_hidden4)\n",
    "            mbwh5 = np.copy(weight_hidden5)\n",
    "            mbwo  = np.copy(weight_output)\n",
    "            lr *= error_out.sum() / preloss\n",
    "            preloss = error_out.sum()  \n",
    "        \n",
    "        print('now training ',epoch*100/epochtimes,'%')\n",
    "        print(error_out.sum())\n",
    "        #print(error_out)\n",
    "        #print('\\n',lr * derror_dwh,'\\n', lr * derror_dwo,'\\n')\n",
    "        \n",
    "       \n",
    "    #print(derror_wh, derror_wo)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.332528684084947e-06 -2.6615043024946736e-07 2.4918622510718354e-06\n",
      "  ... 1.0167315636533624e-07 5.725500247247257e-07\n",
      "  -1.3109153850125618e-08]\n",
      " [0.0 0.0 0.0 ... 0.0 0.0 0.0]\n",
      " [1.2910187849475999e-07 -2.4996090842933576e-07 7.69966886040522e-07 ...\n",
      "  -3.1683911713888497e-07 5.34878161131922e-07 2.174430557415135e-09]\n",
      " ...\n",
      " [-1.5753359032838648e-07 -9.65338296624369e-08 2.907327442982959e-07 ...\n",
      "  4.719123679019223e-07 1.2620357842096499e-07 -9.234508721868498e-09]\n",
      " [5.851519821125392e-07 -3.5382843617427515e-07 4.88799636712002e-07 ...\n",
      "  -7.388682217244033e-08 6.39739523228231e-07 -1.4281146857677638e-10]\n",
      " [-9.220846488034109e-08 -1.1460928557092877e-07 3.309461824684215e-07\n",
      "  ... 4.931060102969919e-07 1.5014775479990894e-07 -9.571561903027928e-09]]\n",
      "[[4.109396178412074e-07 1.071569963523081e-06 -4.146961509092993e-07 ...\n",
      "  1.8679445560394833e-09 6.967330088284538e-08 2.3306174861733065e-08]\n",
      " [-6.687590478429579e-09 3.44767670467253e-07 1.5135782332255244e-07 ...\n",
      "  -3.993401111830065e-09 1.075620311520035e-08 1.5308185030806333e-08]\n",
      " [3.964464031587561e-07 5.970652178301788e-07 3.127680241868839e-07 ...\n",
      "  -9.648839836491686e-09 5.121671825968709e-08 1.7662973403640755e-08]\n",
      " ...\n",
      " [6.552633900733513e-07 -1.2269535347228292e-07 5.910848206300816e-07 ...\n",
      "  -1.2647054471624581e-08 7.315488006302082e-08 1.2561439957492332e-08]\n",
      " [1.2487235641263548e-08 -6.491062273732915e-07 8.487745797811532e-07 ...\n",
      "  -1.2796427594568434e-08 -2.647256712535612e-09 6.963842435591917e-09]\n",
      " [-6.7899122924058435e-09 -7.593676494753981e-10 -6.503548966072291e-09\n",
      "  ... 1.5291500620572987e-10 -8.048949164582925e-10\n",
      "  -1.991095349481249e-10]]\n",
      "[[-8.47047199782561e-07 -2.1347299552778947e-09 -5.014943281272152e-07\n",
      "  ... -2.7455741140554386e-09 -7.544135554618654e-08\n",
      "  8.084096475123695e-09]\n",
      " [2.5404598880993834e-07 -2.0180088652326562e-07 -2.537703487051753e-07\n",
      "  ... 9.665180850618212e-11 2.5637771155029876e-07 -6.103429490941443e-08]\n",
      " [-4.4371442165838434e-06 -4.400243105956725e-07 -9.632561335519025e-07\n",
      "  ... -1.3949851796136216e-08 -1.6564016916601635e-06\n",
      "  -5.069373380460952e-07]\n",
      " ...\n",
      " [7.803940333499644e-09 1.1836715659120674e-08 1.8893215385852113e-08 ...\n",
      "  5.490794436275723e-11 -6.3347771637331224e-09 4.937181071724118e-09]\n",
      " [-1.453326345950921e-07 -1.6219291030850804e-09 -1.1108192004485606e-07\n",
      "  ... -3.512245680192757e-10 1.1517352808925947e-09 4.358960144617184e-09]\n",
      " [3.042862633227617e-08 5.86308608754369e-09 9.421178613395971e-09 ...\n",
      "  3.141539538206449e-10 5.537034015545299e-09 4.093396079654479e-09]]\n",
      "[[-5.627932938016648e-09 -1.8600081146581718e-09 -3.0708062908492155e-08\n",
      "  ... -3.8495377439507554e-10 9.652008883282012e-09 3.769374915442321e-09]\n",
      " [-2.2176422401667677e-08 -7.329214137415072e-09 -3.0298718920474887e-09\n",
      "  ... -1.516879755212603e-09 -2.0251366160283785e-08\n",
      "  1.4852922243481613e-08]\n",
      " [-8.248350917771169e-08 -2.726045359741592e-08 -5.170478916082804e-07\n",
      "  ... -5.64191978072549e-09 1.4599834882487592e-07 5.5244309958854694e-08]\n",
      " ...\n",
      " [5.244971731067896e-10 1.7334410593651133e-10 2.360939974892389e-09 ...\n",
      "  3.587590217926348e-11 -1.3018035496552364e-09 -3.5128821009167976e-10]\n",
      " [-3.398673514314411e-08 -1.1232472731099256e-08 -2.0950123846477256e-08\n",
      "  ... -2.324711792866781e-09 -3.1767115909618434e-08\n",
      "  2.2763019448215636e-08]\n",
      " [-2.6683720793940093e-09 -8.818857144019875e-10 1.0554314828512357e-09\n",
      "  ... -1.8251813875634785e-10 -2.9430070134810935e-09\n",
      "  1.7871738847880802e-09]]\n",
      "[[-2.2252730344139568e-07 1.714899161385772e-09]\n",
      " [-1.3911644587083218e-07 1.0720944015052122e-09]\n",
      " [3.141794963076138e-07 -2.4211962951656367e-09]\n",
      " [-1.2654002296830985e-07 9.751783125592836e-10]\n",
      " [-1.3562237479829298e-07 1.0451690249634613e-09]\n",
      " [-2.2723459730846283e-07 1.7511767116488805e-09]\n",
      " [-4.478526388059215e-08 3.451315437985389e-10]\n",
      " [-1.2994027369155809e-07 1.0013790657434234e-09]\n",
      " [1.0281211718173504e-06 -7.92320476088532e-09]\n",
      " [-1.0149993221050794e-07 7.822036545738468e-10]\n",
      " [-4.1812491543469036e-08 3.2222674242905106e-10]\n",
      " [5.596263684860584e-06 -4.312746482062854e-08]\n",
      " [1.943079993537712e-05 -1.4974314589446075e-07]\n",
      " [-1.0776450994329184e-07 8.304833939996644e-10]\n",
      " [-1.9880543828335003e-07 1.5320847816947063e-09]\n",
      " [-1.811395412967378e-06 1.395954787486203e-08]\n",
      " [1.7315103132638149e-06 -1.3343759704438896e-08]\n",
      " [-2.2307861194816153e-07 1.7191519266159722e-09]\n",
      " [4.9997989803900604e-06 -3.85307634017905e-08]\n",
      " [2.6010103736015585e-09 -2.0046698989983287e-11]\n",
      " [1.1261676949869312e-05 -8.678778761210458e-08]\n",
      " [3.853179810379859e-06 -2.9694486427556345e-08]\n",
      " [-1.6084799911526695e-07 1.2395650621431514e-09]\n",
      " [-4.621129748242116e-07 3.561522233365618e-09]\n",
      " [1.4698319125033437e-05 -1.1327193982707197e-07]\n",
      " [-1.3550085857292021e-08 1.0449238072261054e-10]\n",
      " [-8.567154391425991e-08 6.602244344890693e-10]\n",
      " [1.9666327201960156e-07 -1.5155850738379566e-09]\n",
      " [2.5214899332532608e-05 -1.9431763180932316e-07]\n",
      " [2.2417212954494542e-06 -1.727579960064481e-08]\n",
      " [1.676531519117162e-05 -1.2920124642406764e-07]\n",
      " [-1.0700527371732398e-07 8.246317616139297e-10]\n",
      " [-1.684271901491893e-08 1.2985649584592804e-10]\n",
      " [-9.86698299725547e-08 7.603970798722464e-10]]\n",
      "[[8.768978390342301e-06]\n",
      " [4.2692809447024655e-08]]\n",
      "Elapsed (with compilation) = 726.1934552192688\n"
     ]
    }
   ],
   "source": [
    "#print(weight_hidden)\n",
    "print(lr * derror_dwh)\n",
    "print(lr * derror_dwh2)\n",
    "print(lr * derror_dwh3)\n",
    "print(lr * derror_dwh4)\n",
    "print(lr * derror_dwh5)\n",
    "print(lr * derror_dwo)\n",
    "print(\"Elapsed (with compilation) = %s\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.781202451596185e-05\n"
     ]
    }
   ],
   "source": [
    "single_point = np.array(X_train)\n",
    "result1 = np.dot(single_point, weight_hidden)\n",
    "result2 = ReLU(result1)\n",
    "result3 = np.dot(result2, weight_hidden2)\n",
    "result4 = ReLU(result3)\n",
    "result5 = np.dot(result4, weight_hidden3)\n",
    "result6 = ReLU(result5)\n",
    "result7 = np.dot(result6, weight_hidden4)\n",
    "result8 = ReLU(result7)\n",
    "result9 = np.dot(result8, weight_hidden5)\n",
    "result10 = ReLU(result9)\n",
    "result11 = np.dot(result10, weight_output)\n",
    "result12 = ReLU(result11)\n",
    "#print(result12)\n",
    "#print(Y_train_random)\n",
    "error_out = ((1/len(Y_train)) * (np.power((result12 - Y_train), 2)))\n",
    "#print(error_out)\n",
    "print(error_out.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.35825215e-01]\n",
      " [ 1.06385830e+00]\n",
      " [ 6.18726043e-01]\n",
      " [ 1.26757444e+00]\n",
      " [ 1.24320329e+00]\n",
      " [-2.11839353e-05]\n",
      " [-1.41108043e-05]\n",
      " [ 6.96923388e-01]\n",
      " [-1.13255335e-04]\n",
      " [ 9.83500060e-01]\n",
      " [ 7.58435610e-01]\n",
      " [ 1.41730956e+00]\n",
      " [-1.46744575e-04]\n",
      " [ 9.36763619e-01]\n",
      " [-1.36812793e-02]\n",
      " [ 4.40815423e-01]\n",
      " [ 9.87761846e-01]\n",
      " [ 6.16349806e-01]\n",
      " [ 2.99882428e-02]\n",
      " [ 8.90503951e-01]\n",
      " [ 8.58074516e-01]\n",
      " [ 9.69879044e-01]\n",
      " [-5.43712753e-04]\n",
      " [ 1.03475941e+00]\n",
      " [ 1.07516267e+00]\n",
      " [ 9.45061276e-01]\n",
      " [-6.07096435e-06]\n",
      " [ 1.12514406e+00]\n",
      " [ 1.78771503e-02]\n",
      " [ 1.06357291e+00]\n",
      " [ 1.05161261e+00]\n",
      " [ 1.03283352e+00]\n",
      " [ 1.53659896e-02]\n",
      " [ 1.03713719e+00]\n",
      " [-1.39381323e-02]\n",
      " [ 8.70475462e-01]\n",
      " [ 5.98317036e-01]\n",
      " [ 5.60716599e-01]\n",
      " [ 5.05544495e-01]\n",
      " [ 9.44858106e-01]\n",
      " [ 9.88907823e-01]\n",
      " [ 6.12744152e-01]\n",
      " [-1.05150432e-02]\n",
      " [-3.35183824e-03]\n",
      " [ 8.54759917e-01]\n",
      " [ 1.00179720e+00]\n",
      " [-4.27168797e-04]\n",
      " [ 7.54892612e-02]\n",
      " [ 1.66158832e-04]\n",
      " [ 7.87544220e-01]\n",
      " [ 9.88140771e-01]\n",
      " [-1.83686180e-04]\n",
      " [ 1.55760879e-01]\n",
      " [ 9.98601369e-01]\n",
      " [-8.20549176e-03]\n",
      " [ 9.22045231e-01]\n",
      " [ 1.01493823e+00]\n",
      " [ 7.74094275e-01]\n",
      " [ 1.00467428e+00]\n",
      " [ 9.33141273e-01]\n",
      " [ 1.02762087e+00]\n",
      " [ 9.48298264e-01]\n",
      " [-1.41108043e-05]\n",
      " [-2.72402597e-05]\n",
      " [ 1.49706582e-01]\n",
      " [ 9.91122885e-01]\n",
      " [ 9.40787089e-01]\n",
      " [ 4.07175111e-03]\n",
      " [-4.69849500e-03]\n",
      " [ 1.04781956e+00]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "70\n",
      "0.28835375226518933\n"
     ]
    }
   ],
   "source": [
    "single_point = np.array(X_test)\n",
    "result1 = np.array(np.dot(single_point, weight_hidden), dtype = np.float )\n",
    "result2 = ReLU(result1)\n",
    "result3 = np.array(np.dot(result2, weight_hidden2), dtype = np.float )\n",
    "result4 = ReLU(result3)\n",
    "result5 = np.array(np.dot(result4, weight_hidden3), dtype = np.float )\n",
    "result6 = ReLU(result5)\n",
    "result7 = np.array(np.dot(result6, weight_hidden4), dtype = np.float )\n",
    "result8 = ReLU(result7)\n",
    "result9 = np.array(np.dot(result8, weight_hidden5), dtype = np.float )\n",
    "result10 = ReLU(result9)\n",
    "result11 = np.array(np.dot(result10, weight_output), dtype = np.float )\n",
    "result12 = ReLU(result11)\n",
    "print(result12)\n",
    "print(Y_test)\n",
    "print(len(X_test))\n",
    "error_out = ((1/len(X_test)) * (np.power((result12 - Y_test), 2)))\n",
    "print(math.sqrt(error_out.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bestloss1 =  1e+30\n",
      "0.11803943564332599\n"
     ]
    }
   ],
   "source": [
    "single_point = np.array(X_train)\n",
    "result1 = np.dot(single_point, mbwh1)\n",
    "result2 = ReLU(result1)\n",
    "result3 = np.dot(result2, mbwh2)\n",
    "result4 = ReLU(result3)\n",
    "result5 = np.dot(result4, mbwh3)\n",
    "result6 = ReLU(result5)\n",
    "result7 = np.dot(result6, mbwh4)\n",
    "result8 = ReLU(result7)\n",
    "result9 = np.dot(result8, mbwh5)\n",
    "result10 = ReLU(result9)\n",
    "result11 = np.dot(result10, mbwo)\n",
    "result12 = ReLU(result11)\n",
    "print(\"bestloss1 = \", bestloss1)\n",
    "#print(result12)\n",
    "#print(Y_train_random)\n",
    "error_out = ((1/len(Y_train)) * (np.power((result12 - Y_train), 2)))\n",
    "print(error_out.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
