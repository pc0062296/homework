{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas \n",
    "import random \n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pandas.read_csv(\"ionosphere_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1  0  0.99539  -0.05889  0.85243  0.02306  0.83398  -0.37708      1.1  \\\n",
      "0  1  0  1.00000  -0.18829  0.93035 -0.36156 -0.10868  -0.93597  1.00000   \n",
      "1  1  0  1.00000  -0.03365  1.00000  0.00485  1.00000  -0.12062  0.88965   \n",
      "2  1  0  1.00000  -0.45161  1.00000  1.00000  0.71216  -1.00000  0.00000   \n",
      "3  1  0  1.00000  -0.02401  0.94140  0.06531  0.92106  -0.23255  0.77152   \n",
      "4  1  0  0.02337  -0.00592 -0.09924 -0.11949 -0.00763  -0.11824  0.14706   \n",
      "\n",
      "   0.03760  ...  -0.51171  0.41078  -0.46168  0.21266  -0.34090  0.42267  \\\n",
      "0 -0.04549  ...  -0.26569 -0.20468  -0.18401 -0.19040  -0.11593 -0.16626   \n",
      "1  0.01198  ...  -0.40220  0.58984  -0.22145  0.43100  -0.17365  0.60436   \n",
      "2  0.00000  ...   0.90695  0.51613   1.00000  1.00000  -0.20099  0.25682   \n",
      "3 -0.16399  ...  -0.65158  0.13290  -0.53206  0.02431  -0.62197 -0.05707   \n",
      "4  0.06637  ...  -0.01535 -0.03240   0.09223 -0.07859   0.00732  0.00000   \n",
      "\n",
      "   -0.54487  0.18641  -0.45300  g  \n",
      "0  -0.06288 -0.13738  -0.02447  b  \n",
      "1  -0.24180  0.56045  -0.38238  g  \n",
      "2   1.00000 -0.32382   1.00000  b  \n",
      "3  -0.59573 -0.04608  -0.65697  g  \n",
      "4   0.00000 -0.00039   0.12011  b  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                1      0     0.99539    -0.05889     0.85243     0.02306  \\\n",
      "count  350.000000  350.0  350.000000  350.000000  350.000000  350.000000   \n",
      "mean     0.891429    0.0    0.640330    0.044667    0.600350    0.116154   \n",
      "std      0.311546    0.0    0.498059    0.442032    0.520431    0.461443   \n",
      "min      0.000000    0.0   -1.000000   -1.000000   -1.000000   -1.000000   \n",
      "25%      1.000000    0.0    0.471518   -0.065388    0.412555   -0.024868   \n",
      "50%      1.000000    0.0    0.870795    0.016700    0.808620    0.021170   \n",
      "75%      1.000000    0.0    1.000000    0.194727    1.000000    0.335318   \n",
      "max      1.000000    0.0    1.000000    1.000000    1.000000    1.000000   \n",
      "\n",
      "          0.83398    -0.37708         1.1     0.03760  ...     0.56811  \\\n",
      "count  350.000000  350.000000  350.000000  350.000000  ...  350.000000   \n",
      "mean     0.549284    0.120779    0.510453    0.181756  ...    0.395643   \n",
      "std      0.493124    0.520816    0.507117    0.484482  ...    0.579206   \n",
      "min     -1.000000   -1.000000   -1.000000   -1.000000  ...   -1.000000   \n",
      "25%      0.209105   -0.053483    0.086785   -0.049003  ...    0.000000   \n",
      "50%      0.728000    0.015085    0.682430    0.017550  ...    0.549175   \n",
      "75%      0.970445    0.451572    0.950555    0.536192  ...    0.907165   \n",
      "max      1.000000    1.000000    1.000000    1.000000  ...    1.000000   \n",
      "\n",
      "         -0.51171     0.41078    -0.46168     0.21266    -0.34090     0.42267  \\\n",
      "count  350.000000  350.000000  350.000000  350.000000  350.000000  350.000000   \n",
      "mean    -0.069928    0.542015   -0.068417    0.378919   -0.027013    0.352313   \n",
      "std      0.508675    0.516896    0.550411    0.576642    0.508425    0.572289   \n",
      "min     -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   \n",
      "25%     -0.323745    0.283612   -0.428992    0.000000   -0.234935    0.000000   \n",
      "50%     -0.014915    0.708530   -0.017685    0.499215    0.000000    0.446875   \n",
      "75%      0.157922    0.999972    0.154862    0.884572    0.154218    0.859490   \n",
      "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
      "\n",
      "         -0.54487     0.18641    -0.45300  \n",
      "count  350.000000  350.000000  350.000000  \n",
      "mean    -0.002248    0.349829    0.015816  \n",
      "std      0.513491    0.523339    0.468338  \n",
      "min     -1.000000   -1.000000   -1.000000  \n",
      "25%     -0.239347    0.000000   -0.161013  \n",
      "50%      0.000000    0.413115    0.000000  \n",
      "75%      0.200935    0.816777    0.172105  \n",
      "max      1.000000    1.000000    1.000000  \n",
      "\n",
      "[8 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataframe.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (350, 35)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape:\", dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types: 1              int64\n",
      "0              int64\n",
      "0.99539      float64\n",
      "-0.05889     float64\n",
      "0.85243      float64\n",
      "0.02306      float64\n",
      "0.83398      float64\n",
      "-0.37708     float64\n",
      "1.1          float64\n",
      "0.03760      float64\n",
      "0.85243.1    float64\n",
      "-0.17755     float64\n",
      "0.59755      float64\n",
      "-0.44945     float64\n",
      "0.60536      float64\n",
      "-0.38223     float64\n",
      "0.84356      float64\n",
      "-0.38542     float64\n",
      "0.58212      float64\n",
      "-0.32192     float64\n",
      "0.56971      float64\n",
      "-0.29674     float64\n",
      "0.36946      float64\n",
      "-0.47357     float64\n",
      "0.56811      float64\n",
      "-0.51171     float64\n",
      "0.41078      float64\n",
      "-0.46168     float64\n",
      "0.21266      float64\n",
      "-0.34090     float64\n",
      "0.42267      float64\n",
      "-0.54487     float64\n",
      "0.18641      float64\n",
      "-0.45300     float64\n",
      "g             object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Types:\", dataframe.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1.0 ... -0.13738 -0.02447 'b']\n",
      " [1 0 1.0 ... 0.56045 -0.38238 'g']\n",
      " [1 0 1.0 ... -0.32382 1.0 'b']\n",
      " ...\n",
      " [1 0 0.94701 ... 0.9269700000000001 -0.00577 'g']\n",
      " [1 0 0.9060799999999999 ... 0.87403 -0.16243 'g']\n",
      " [1 0 0.8471 ... 0.85764 -0.06151 'g']]\n"
     ]
    }
   ],
   "source": [
    "print(dataframe.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1.0, ..., 0.29220999999999997, -0.8962100000000001, 'g'],\n",
       "       [0, 0, 1.0, ..., 1.0, -1.0, 'b'],\n",
       "       [1, 0, 0.8225399999999999, ..., 0.7843899999999999, 0.01214, 'g'],\n",
       "       ...,\n",
       "       [1, 0, 0.19466, ..., -0.30153, -0.33588, 'g'],\n",
       "       [1, 0, 1.0, ..., 1.0, 1.0, 'b'],\n",
       "       [0, 0, 1.0, ..., 0.0, 0.0, 'b']], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = np.array(dataframe.values)\n",
    "dataset_shuf = []\n",
    "index_shuf = list(range(len(dataset)))\n",
    "random.shuffle(index_shuf)\n",
    "for i in index_shuf:\n",
    "    dataset_shuf.append(dataset[i,:])   \n",
    "dataset_shuf = np.array(dataset_shuf)\n",
    "dataset_shuf.reshape(350,35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['g'],\n",
       "       ['b'],\n",
       "       ['b']], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(dataset_shuf[:280,0:34], dtype = np.float)\n",
    "Y_train = np.array(dataset_shuf[:280,34:35], dtype = np.object)\n",
    "X_train.reshape(280,34)\n",
    "Y_train.reshape(280,1)\n",
    "\n",
    "X_test = np.array(dataset_shuf[280:,0:34], dtype = np.float)\n",
    "Y_test = np.array(dataset_shuf[280:,34:35], dtype = np.object)\n",
    "X_test.reshape(70,34)\n",
    "Y_test.reshape(70,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "\n",
    "Y_train[:,0] = labelencoder.fit_transform(Y_train[:, 0])\n",
    "Y_test[:,0] = labelencoder.fit_transform(Y_test[:, 0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.     ,  0.     ,  1.     , ..., -0.88846,  0.29221, -0.89621],\n",
       "       [ 0.     ,  0.     ,  1.     , ...,  1.     ,  1.     , -1.     ],\n",
       "       [ 1.     ,  0.     ,  0.82254, ..., -0.0422 ,  0.78439,  0.01214],\n",
       "       ...,\n",
       "       [ 1.     ,  0.     ,  0.28409, ..., -0.70076,  0.     ,  0.     ],\n",
       "       [ 1.     ,  0.     ,  0.39394, ...,  0.0641 ,  0.39394,  0.24242],\n",
       "       [ 1.     ,  0.     ,  0.97714, ..., -0.33662, -0.59943, -0.497  ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.DataFrame(X_train)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define weight:\n",
    "\n",
    "#7 for hidden layer1\n",
    "#5 for hidden layer2\n",
    "#2 for output layer\n",
    "#9 total\n",
    "\n",
    "weight_hidden = np.random.random((34,68))-0.5\n",
    "weight_hidden2 = np.random.random((68,34))-0.5\n",
    "weight_hidden3 = np.random.random((34,34))-0.5\n",
    "weight_hidden4 = np.random.random((34,34))-0.5\n",
    "weight_hidden5 = np.random.random((34,2))-0.5\n",
    "weight_output = np.random.random((2,1))-0.5\n",
    "lr = 0.003\n",
    "bias = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x) :\n",
    "    return x * (x > 0) + 0.01 * x * (x <= 0)\n",
    "\n",
    "def ReLU_der(x) :\n",
    "    return 1 * (x > 0) + 0.01 * (x <= 0)\n",
    "\n",
    "def Sigmoid(x) :\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def Sigmoid_der(x) :\n",
    "    return Sigmoid(x) * (1 - Sigmoid(x))\n",
    "\n",
    "def limit(x) :  \n",
    "    while(np.max(x) > 0.1 or np.min(x) < -0.1) : \n",
    "            x /= 10\n",
    "    return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now training  0.0 %\n",
      "54.342270935686486\n",
      "now training  0.1 %\n",
      "18.510098377059876\n",
      "now training  0.2 %\n",
      "7.951528413111361\n",
      "now training  0.3 %\n",
      "6.253848306119536\n",
      "now training  0.4 %\n",
      "6.5173321790486485\n",
      "now training  0.5 %\n",
      "4.855494455099366\n",
      "now training  0.6 %\n",
      "4.020714917454211\n",
      "now training  0.7 %\n",
      "3.4114827255474003\n",
      "now training  0.8 %\n",
      "2.9111341976136877\n",
      "now training  0.9 %\n",
      "2.8360791446579117\n",
      "now training  1.0 %\n",
      "2.2980903234398657\n",
      "now training  1.1 %\n",
      "2.1100898654808735\n",
      "now training  1.2 %\n",
      "1.8763593256607976\n",
      "now training  1.3 %\n",
      "1.868916717920387\n",
      "now training  1.4 %\n",
      "1.5358620113598598\n",
      "now training  1.5 %\n",
      "1.476662718135873\n",
      "now training  1.6 %\n",
      "1.725786834330309\n",
      "now training  1.7 %\n",
      "1.2895755469808263\n",
      "now training  1.8 %\n",
      "1.2573840577784816\n",
      "now training  1.9 %\n",
      "1.4260103798475936\n",
      "now training  2.0 %\n",
      "1.1639914753443117\n",
      "now training  2.1 %\n",
      "1.039380798920618\n",
      "now training  2.2 %\n",
      "1.1499437647551716\n",
      "now training  2.3 %\n",
      "0.9511595284545573\n",
      "now training  2.4 %\n",
      "1.1688229421538443\n",
      "now training  2.5 %\n",
      "0.9328697823200536\n",
      "now training  2.6 %\n",
      "0.9823592080582214\n",
      "now training  2.7 %\n",
      "0.8589811932952802\n",
      "now training  2.8 %\n",
      "0.9078046584280042\n",
      "now training  2.9 %\n",
      "0.7939744610182747\n",
      "now training  3.0 %\n",
      "0.8161369429888308\n",
      "now training  3.1 %\n",
      "0.7730896615231461\n",
      "now training  3.2 %\n",
      "0.7619906042430931\n",
      "now training  3.3 %\n",
      "0.7570085364919948\n",
      "now training  3.4 %\n",
      "0.7262965694520377\n",
      "now training  3.5 %\n",
      "0.76082634997442\n",
      "now training  3.6 %\n",
      "0.7159046504112433\n",
      "now training  3.7 %\n",
      "0.7518093640704346\n",
      "now training  3.8 %\n",
      "0.6923427709954406\n",
      "now training  3.9 %\n",
      "0.7283964172086882\n",
      "now training  4.0 %\n",
      "0.6987101876678423\n",
      "now training  4.1 %\n",
      "0.6665659480093182\n",
      "now training  4.2 %\n",
      "0.6685376996174333\n",
      "now training  4.3 %\n",
      "0.6464500494051092\n",
      "now training  4.4 %\n",
      "0.6503346083249372\n",
      "now training  4.5 %\n",
      "0.6332726616938062\n",
      "now training  4.6 %\n",
      "0.6303725045288715\n",
      "now training  4.7 %\n",
      "0.6322528175823728\n",
      "now training  4.8 %\n",
      "0.6176224959272977\n",
      "now training  4.9 %\n",
      "0.620967971339254\n",
      "now training  5.0 %\n",
      "0.6085847627905493\n",
      "now training  5.1 %\n",
      "0.6133416434170388\n",
      "now training  5.2 %\n",
      "0.6018616560111089\n",
      "now training  5.3 %\n",
      "0.5983756878736262\n",
      "now training  5.4 %\n",
      "0.5998088512147203\n",
      "now training  5.5 %\n",
      "0.5922149363509901\n",
      "now training  5.6 %\n",
      "0.6013488129706543\n",
      "now training  5.7 %\n",
      "0.587453056375486\n",
      "now training  5.8 %\n",
      "0.606211039595169\n",
      "now training  5.9 %\n",
      "0.5817514270520877\n",
      "now training  6.0 %\n",
      "0.5797104418372342\n",
      "now training  6.1 %\n",
      "0.57900079104298\n",
      "now training  6.2 %\n",
      "0.5755404668349894\n",
      "now training  6.3 %\n",
      "0.5740640739523268\n",
      "now training  6.4 %\n",
      "0.5701278426108284\n",
      "now training  6.5 %\n",
      "0.5795878262840126\n",
      "now training  6.6 %\n",
      "0.5708345833404839\n",
      "now training  6.7 %\n",
      "0.6284591040909803\n",
      "now training  6.8 %\n",
      "0.5707791673159661\n",
      "now training  6.9 %\n",
      "0.5681380861655744\n",
      "now training  7.0 %\n",
      "0.5609693271271486\n",
      "now training  7.1 %\n",
      "0.5576358628459884\n",
      "now training  7.2 %\n",
      "0.56307916009162\n",
      "now training  7.3 %\n",
      "0.5556784760254484\n",
      "now training  7.4 %\n",
      "0.5529087306462545\n",
      "now training  7.5 %\n",
      "0.5521126867274141\n",
      "now training  7.6 %\n",
      "0.5538710886097952\n",
      "now training  7.7 %\n",
      "0.5488696019167909\n",
      "now training  7.8 %\n",
      "0.5493026463327366\n",
      "now training  7.9 %\n",
      "0.5531073784967898\n",
      "now training  8.0 %\n",
      "0.5466710463515706\n",
      "now training  8.1 %\n",
      "0.5437513986653176\n",
      "now training  8.2 %\n",
      "0.5535767592140648\n",
      "now training  8.3 %\n",
      "0.5481776221014983\n",
      "now training  8.4 %\n",
      "0.5407232110014822\n",
      "now training  8.5 %\n",
      "0.5403893443395472\n",
      "now training  8.6 %\n",
      "0.5382345458742912\n",
      "now training  8.7 %\n",
      "0.5388855468601861\n",
      "now training  8.8 %\n",
      "0.5360148080783305\n",
      "now training  8.9 %\n",
      "0.5352564268697272\n",
      "now training  9.0 %\n",
      "0.5358911081749564\n",
      "now training  9.1 %\n",
      "0.5336396133618803\n",
      "now training  9.2 %\n",
      "0.5329221825193983\n",
      "now training  9.3 %\n",
      "0.5322865049177461\n",
      "now training  9.4 %\n",
      "0.5319547862204232\n",
      "now training  9.5 %\n",
      "0.5325014631592012\n",
      "now training  9.6 %\n",
      "0.5369529353211429\n",
      "now training  9.7 %\n",
      "0.5293528374232478\n",
      "now training  9.8 %\n",
      "0.5287380429596911\n",
      "now training  9.9 %\n",
      "0.5279566194169572\n",
      "now training  10.0 %\n",
      "0.5304154180650056\n",
      "now training  10.1 %\n",
      "0.5277718193560602\n",
      "now training  10.2 %\n",
      "0.5268204509966011\n",
      "now training  10.3 %\n",
      "0.5269199097289685\n",
      "now training  10.4 %\n",
      "0.5264784071089413\n",
      "now training  10.5 %\n",
      "0.5246791478337017\n",
      "now training  10.6 %\n",
      "0.5239660508585131\n",
      "now training  10.7 %\n",
      "0.5247281404238424\n",
      "now training  10.8 %\n",
      "0.5252031623204757\n",
      "now training  10.9 %\n",
      "0.5228780535444534\n",
      "now training  11.0 %\n",
      "0.5232775283024488\n",
      "now training  11.1 %\n",
      "0.5228564459554189\n",
      "now training  11.2 %\n",
      "0.5210362710861685\n",
      "now training  11.3 %\n",
      "0.5216271242813431\n",
      "now training  11.4 %\n",
      "0.520425230935505\n",
      "now training  11.5 %\n",
      "0.5208319006130129\n",
      "now training  11.6 %\n",
      "0.5197044073627409\n",
      "now training  11.7 %\n",
      "0.5202115332009488\n",
      "now training  11.8 %\n",
      "0.5190899696480692\n",
      "now training  11.9 %\n",
      "0.5188328431205879\n",
      "now training  12.0 %\n",
      "0.5185036121277468\n",
      "now training  12.1 %\n",
      "0.5201670472930923\n",
      "now training  12.2 %\n",
      "0.5199246976968892\n",
      "now training  12.3 %\n",
      "0.5184248648876463\n",
      "now training  12.4 %\n",
      "0.5188447899847338\n",
      "now training  12.5 %\n",
      "0.5190909845429738\n",
      "now training  12.6 %\n",
      "0.5187009210582358\n",
      "now training  12.7 %\n",
      "0.5172833755768326\n",
      "now training  12.8 %\n",
      "0.5178869202585188\n",
      "now training  12.9 %\n",
      "0.5226903734125866\n",
      "now training  13.0 %\n",
      "0.5176967951064507\n",
      "now training  13.1 %\n",
      "0.518289638467343\n",
      "now training  13.2 %\n",
      "0.5169513877993601\n",
      "now training  13.3 %\n",
      "0.5162482845959256\n",
      "now training  13.4 %\n",
      "0.5163565260983295\n",
      "now training  13.5 %\n",
      "0.5167180213294872\n",
      "now training  13.6 %\n",
      "0.5160132111302972\n",
      "now training  13.7 %\n",
      "0.515750536183442\n",
      "now training  13.8 %\n",
      "0.5164811281685097\n",
      "now training  13.9 %\n",
      "0.5154513442066223\n",
      "now training  14.0 %\n",
      "0.5162122334312894\n",
      "now training  14.1 %\n",
      "0.5152489682251518\n",
      "now training  14.2 %\n",
      "0.515735435168997\n",
      "now training  14.3 %\n",
      "0.5157029039882556\n",
      "now training  14.4 %\n",
      "0.5156278162869298\n",
      "now training  14.5 %\n",
      "0.515319013953433\n",
      "now training  14.6 %\n",
      "0.5160154394692349\n",
      "now training  14.7 %\n",
      "0.5164177762121567\n",
      "now training  14.8 %\n",
      "0.5149442240478173\n",
      "now training  14.9 %\n",
      "0.5146655732611491\n",
      "now training  15.0 %\n",
      "0.5148333480221547\n",
      "now training  15.1 %\n",
      "0.5144738543895684\n",
      "now training  15.2 %\n",
      "0.515595684127712\n",
      "now training  15.3 %\n",
      "0.5144214560799456\n",
      "now training  15.4 %\n",
      "0.5143500429732356\n",
      "now training  15.5 %\n",
      "0.5143798731818313\n",
      "now training  15.6 %\n",
      "0.514243023465878\n",
      "now training  15.7 %\n",
      "0.5141308169966926\n",
      "now training  15.8 %\n",
      "0.5142224960970359\n",
      "now training  15.9 %\n",
      "0.514070339938389\n",
      "now training  16.0 %\n",
      "0.5140246783900643\n",
      "now training  16.1 %\n",
      "0.514176430245398\n",
      "now training  16.2 %\n",
      "0.5144722875205429\n",
      "now training  16.3 %\n",
      "0.5138881782709056\n",
      "now training  16.4 %\n",
      "0.5139638931482191\n",
      "now training  16.5 %\n",
      "0.5138168291306856\n",
      "now training  16.6 %\n",
      "0.514194609983714\n",
      "now training  16.7 %\n",
      "0.5140666423222578\n",
      "now training  16.8 %\n",
      "0.5139040931398501\n",
      "now training  16.9 %\n",
      "0.5137343732127287\n",
      "now training  17.0 %\n",
      "0.5140038979997509\n",
      "now training  17.1 %\n",
      "0.5139643554091602\n",
      "now training  17.2 %\n",
      "0.5136935709285902\n",
      "now training  17.3 %\n",
      "0.5138141133362177\n",
      "now training  17.4 %\n",
      "0.513719563654814\n",
      "now training  17.5 %\n",
      "0.5135700149606095\n",
      "now training  17.6 %\n",
      "0.5136038214778358\n",
      "now training  17.7 %\n",
      "0.5136294422296291\n",
      "now training  17.8 %\n",
      "0.5135759468694041\n",
      "now training  17.9 %\n",
      "0.5135147295739639\n",
      "now training  18.0 %\n",
      "0.5137418511251625\n",
      "now training  18.1 %\n",
      "0.5139150259576619\n",
      "now training  18.2 %\n",
      "0.5135269776739766\n",
      "now training  18.3 %\n",
      "0.5135858858576078\n",
      "now training  18.4 %\n",
      "0.513621180471505\n",
      "now training  18.5 %\n",
      "0.5136592269298241\n",
      "now training  18.6 %\n",
      "0.5135361002062219\n",
      "now training  18.7 %\n",
      "0.5135562948591346\n",
      "now training  18.8 %\n",
      "0.5135619957155556\n",
      "now training  18.9 %\n",
      "0.5134387327347529\n",
      "now training  19.0 %\n",
      "0.5134367713298716\n",
      "now training  19.1 %\n",
      "0.513521787816419\n",
      "now training  19.2 %\n",
      "0.5134614998335345\n",
      "now training  19.3 %\n",
      "0.5134336174759031\n",
      "now training  19.4 %\n",
      "0.5134442886522577\n",
      "now training  19.5 %\n",
      "0.5133995829112555\n",
      "now training  19.6 %\n",
      "0.5133484938613984\n",
      "now training  19.7 %\n",
      "0.513351100735099\n",
      "now training  19.8 %\n",
      "0.5133301497155581\n",
      "now training  19.9 %\n",
      "0.5133288262520193\n",
      "now training  20.0 %\n",
      "0.5133281751209202\n",
      "now training  20.1 %\n",
      "0.5134009646691112\n",
      "now training  20.2 %\n",
      "0.5133639149063323\n",
      "now training  20.3 %\n",
      "0.5133560625189505\n",
      "now training  20.4 %\n",
      "0.5133377142799399\n",
      "now training  20.5 %\n",
      "0.5133417228201059\n",
      "now training  20.6 %\n",
      "0.5133462267805778\n",
      "now training  20.7 %\n",
      "0.5133079719283549\n",
      "now training  20.8 %\n",
      "0.5132904868747364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now training  20.9 %\n",
      "0.5132891751380224\n",
      "now training  21.0 %\n",
      "0.5132597815864295\n",
      "now training  21.1 %\n",
      "0.5132472256763594\n",
      "now training  21.2 %\n",
      "0.513230487835084\n",
      "now training  21.3 %\n",
      "0.5132097966389767\n",
      "now training  21.4 %\n",
      "0.5131983063807646\n",
      "now training  21.5 %\n",
      "0.5131901086422637\n",
      "now training  21.6 %\n",
      "0.5131994355581154\n",
      "now training  21.7 %\n",
      "0.5131931653942957\n",
      "now training  21.8 %\n",
      "0.5131850086840659\n",
      "now training  21.9 %\n",
      "0.513184653836662\n",
      "now training  22.0 %\n",
      "0.5131793992633278\n",
      "now training  22.1 %\n",
      "0.5131854231435987\n",
      "now training  22.2 %\n",
      "0.513167929072296\n",
      "now training  22.3 %\n",
      "0.513162057454697\n",
      "now training  22.4 %\n",
      "0.51314548237154\n",
      "now training  22.5 %\n",
      "0.5131416264658509\n",
      "now training  22.6 %\n",
      "0.5131345570412141\n",
      "now training  22.7 %\n",
      "0.5131282010607526\n",
      "now training  22.8 %\n",
      "0.5131235053869833\n",
      "now training  22.9 %\n",
      "0.5131214270958092\n",
      "now training  23.0 %\n",
      "0.513117264085464\n",
      "now training  23.1 %\n",
      "0.5131125880517113\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-39a2c3525517>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mdouth2_dinh2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReLU_der\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_hidden2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mdinh2_dwh2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_hidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[0mderror_dwh2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdinh2_dwh2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdouth2_dinh2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mderror_douth2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;31m#==========================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochtimes = 200000\n",
    "mini_batch = 8\n",
    "bestloss1 = 1e30 #Mini Batch Loss\n",
    "preloss = 1e30   #decay the learning rate\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(epochtimes):\n",
    "    X_train_random = []\n",
    "    Y_train_random = []\n",
    "    for i in range(mini_batch):\n",
    "        rindex = random.randint(0,len(X_train)-1)\n",
    "        X_train_random.append(X_train[rindex,:])\n",
    "        Y_train_random.append(Y_train[rindex,:])\n",
    "    X_train_random = np.array(X_train_random)\n",
    "    Y_train_random = np.array(Y_train_random)\n",
    "    \n",
    "        \n",
    "    input_hidden = np.dot(X_train_random, weight_hidden) \n",
    "    output_hidden = ReLU(input_hidden)\n",
    "    \n",
    "    input_hidden2 = np.dot(output_hidden, weight_hidden2) \n",
    "    output_hidden2 = ReLU(input_hidden2)\n",
    "    input_hidden3 = np.dot(output_hidden2, weight_hidden3) \n",
    "    output_hidden3 = ReLU(input_hidden3)\n",
    "    input_hidden4 = np.dot(output_hidden3, weight_hidden4) \n",
    "    output_hidden4 = ReLU(input_hidden4)\n",
    "    input_hidden5 = np.dot(output_hidden4, weight_hidden5) \n",
    "    output_hidden5 = ReLU(input_hidden5)\n",
    "    input_op = np.dot(output_hidden5, weight_output)\n",
    "    output_op = ReLU(input_op)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #==========================================\n",
    "    \n",
    "    derror_douto = output_op - Y_train_random\n",
    "    douto_dino = ReLU_der(input_op)\n",
    "    dino_dwo = output_hidden5      \n",
    "    derror_dwo = np.dot(dino_dwo.T, derror_douto * douto_dino)\n",
    "    \n",
    "    #==========================================\n",
    "    \n",
    "    derror_dino = derror_douto * douto_dino\n",
    "    dino_douth5 = weight_output\n",
    "    derror_douth5 = np.dot(derror_dino, dino_douth5.T)\n",
    "    douth5_dinh5 = ReLU_der(input_hidden5)\n",
    "    dinh5_dwh5 = output_hidden4\n",
    "    derror_dwh5 = np.dot(dinh5_dwh5.T, douth5_dinh5 * derror_douth5)\n",
    "    \n",
    "    #==========================================\n",
    "    \n",
    "    derror_dinh5 = derror_douth5 * douth5_dinh5\n",
    "    dinh5_douth4 = weight_hidden5\n",
    "    derror_douth4 = np.dot(derror_dinh5, dinh5_douth4.T)\n",
    "    douth4_dinh4 = ReLU_der(input_hidden4)\n",
    "    dinh4_dwh4 = output_hidden3\n",
    "    derror_dwh4 = np.dot(dinh4_dwh4.T, douth4_dinh4 * derror_douth4)\n",
    "    \n",
    "    #==========================================\n",
    "    \n",
    "    derror_dinh4 = derror_douth4 * douth4_dinh4\n",
    "    dinh4_douth3 = weight_hidden4\n",
    "    derror_douth3 = np.dot(derror_dinh4, dinh4_douth3.T)\n",
    "    douth3_dinh3 = ReLU_der(input_hidden3)\n",
    "    dinh3_dwh3 = output_hidden2\n",
    "    derror_dwh3 = np.dot(dinh3_dwh3.T, douth3_dinh3 * derror_douth3)\n",
    "    \n",
    "    #==========================================\n",
    "    \n",
    "    derror_dinh3 = derror_douth3 * douth3_dinh3\n",
    "    dinh3_douth2 = weight_hidden3\n",
    "    derror_douth2 = np.dot(derror_dinh3, dinh3_douth2.T)\n",
    "    douth2_dinh2 = ReLU_der(input_hidden2)\n",
    "    dinh2_dwh2 = output_hidden\n",
    "    derror_dwh2 = np.dot(dinh2_dwh2.T, douth2_dinh2 * derror_douth2)\n",
    "    \n",
    "    #==========================================\n",
    "    \n",
    "    derror_dinh2 = derror_douth2 * douth2_dinh2\n",
    "    dinh2_douth = weight_hidden2\n",
    "    derror_douth = np.dot(derror_dinh2, dinh2_douth.T)\n",
    "    douth_dinh = ReLU_der(input_hidden)\n",
    "    dinh_dwh = X_train_random\n",
    "    derror_dwh = np.dot(dinh_dwh.T, douth_dinh * derror_douth)\n",
    "    \n",
    "    #==========================================\n",
    "       \n",
    "    \n",
    "    \n",
    "    weight_hidden = weight_hidden - limit(lr * derror_dwh)\n",
    "    weight_hidden2 = weight_hidden2 - limit(lr * derror_dwh2)\n",
    "    weight_hidden3 = weight_hidden3 - limit(lr * derror_dwh3)\n",
    "    weight_hidden4 = weight_hidden4 - limit(lr * derror_dwh4)\n",
    "    weight_hidden5 = weight_hidden5 - limit(lr * derror_dwh5)\n",
    "    weight_output = weight_output - limit(lr * derror_dwo)\n",
    "    \n",
    "    \n",
    "    error_out = ((1 / 2) * (np.power((output_op - Y_train_random), 2)))\n",
    "    #print(error_out.sum())\n",
    "    \n",
    "        \n",
    "        \n",
    "    #calulate error \n",
    "    if epoch % 200 == 0 : \n",
    "        input_hidden = np.dot(X_train, weight_hidden) \n",
    "        output_hidden = ReLU(input_hidden)\n",
    "        input_hidden2 = np.dot(output_hidden, weight_hidden2) \n",
    "        output_hidden2 = ReLU(input_hidden2)\n",
    "        input_hidden3 = np.dot(output_hidden2, weight_hidden3) \n",
    "        output_hidden3 = ReLU(input_hidden3)\n",
    "        input_hidden4 = np.dot(output_hidden3, weight_hidden4) \n",
    "        output_hidden4 = ReLU(input_hidden4)\n",
    "        input_hidden5 = np.dot(output_hidden4, weight_hidden5) \n",
    "        output_hidden5 = ReLU(input_hidden5)\n",
    "        input_op = np.dot(output_hidden5, weight_output)\n",
    "        output_op = ReLU(input_op)\n",
    "        error_out = ((1 / 2) * (np.power((output_op - Y_train), 2)))  \n",
    "            \n",
    "        if error_out.sum() > preloss :\n",
    "            lr *= 0.95\n",
    "        preloss = error_out.sum()  \n",
    "        \n",
    "        print('now training ',epoch*100/epochtimes,'%')\n",
    "        print(error_out.sum())\n",
    "        #print(error_out)\n",
    "        #print('\\n',lr * derror_dwh,'\\n', lr * derror_dwo,'\\n')\n",
    "        \n",
    "       \n",
    "    #print(derror_wh, derror_wo)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.566200765686286e-09 -1.7429297995137973e-09 -1.8780655543412932e-08\n",
      "  ... -2.621587379409494e-10 1.8147968757844272e-09\n",
      "  -3.0887833949948896e-10]\n",
      " [0.0 0.0 0.0 ... 0.0 0.0 0.0]\n",
      " [3.5642676345626742e-09 -1.7398703952368523e-09 -1.8493374484420203e-08\n",
      "  ... -2.5783475775167645e-10 1.818452542789002e-09\n",
      "  -3.0326311273428826e-10]\n",
      " ...\n",
      " [2.244823098150335e-09 1.092492019477202e-09 -6.775087177674219e-09 ...\n",
      "  -6.201487905509645e-11 -1.7180255207006156e-09 -8.879984240018694e-11]\n",
      " [9.137016633104483e-10 -1.646059276301045e-09 -9.860947567782847e-09 ...\n",
      "  -1.465662068839964e-10 1.9501083954660505e-09 -1.482280956091283e-10]\n",
      " [2.659696621989893e-09 1.205675748100872e-09 -8.630815641133439e-09 ...\n",
      "  -8.029942013749166e-11 -1.9367196710510503e-09 -1.0668802814643452e-10]]\n",
      "[[1.8451108632251271e-09 1.4990259522133619e-10 4.862696644671096e-13 ...\n",
      "  -1.3159793232405083e-09 9.190811057706077e-12 3.196716130974016e-10]\n",
      " [-3.14651864515696e-10 -1.0628290103344016e-11 1.2050863854861e-12 ...\n",
      "  3.5550685634256516e-10 -2.306695008802939e-12 -7.115331790446941e-11]\n",
      " [8.123456056023187e-09 2.716479916840751e-10 1.3833159020269328e-11 ...\n",
      "  -4.829992045594472e-08 1.445529718260483e-10 7.307055367893076e-09]\n",
      " ...\n",
      " [-5.286418141431177e-10 -2.068790157058988e-11 6.858547151559363e-13 ...\n",
      "  7.11544567544289e-10 -3.062051478596071e-12 -1.0102414883812653e-10]\n",
      " [-2.33930767896703e-10 -1.598852231901364e-11 1.4166125625098688e-11 ...\n",
      "  1.2147702246407362e-09 -1.9326503850066035e-11 -7.513353612695652e-10]\n",
      " [-1.6499502891467574e-10 -5.893266042621317e-12 1.2282224175932214e-13\n",
      "  ... 4.0382716304729786e-10 -1.286810449686611e-12\n",
      "  -5.937516617063217e-11]]\n",
      "[[-3.892955445212619e-13 -1.5356689193464325e-10 -1.285055010296689e-11\n",
      "  ... -1.2552096856864271e-09 2.995891033261074e-10 7.168288003377297e-10]\n",
      " [1.177006147840596e-12 8.70333345241832e-12 8.831232881316889e-13 ...\n",
      "  2.0570312444231e-11 -5.428465198468008e-12 -1.119835042636527e-11]\n",
      " [-2.9042335982049814e-12 1.295179662465838e-10 -1.4519094093362429e-12\n",
      "  ... 8.810678358065764e-12 -8.857195095723631e-13\n",
      "  -3.3628680637591227e-12]\n",
      " ...\n",
      " [1.9791834392172537e-10 -2.116757515149332e-08 7.68928326738423e-11 ...\n",
      "  -1.796696926075203e-09 3.462305079009603e-10 9.080467732244886e-10]\n",
      " [-9.726175649220973e-13 6.381746119237916e-11 -4.2531013724540125e-13\n",
      "  ... 9.41518679117699e-12 -1.854394584147127e-12 -4.816203459254032e-12]\n",
      " [4.729729987915895e-10 -2.428489982167515e-08 2.379233626385792e-10 ...\n",
      "  -1.5311667785814859e-09 1.7037276517958992e-10 6.031152861448844e-10]]\n",
      "[[-1.002413005233423e-12 -9.333451436631037e-13 3.040672015937555e-10 ...\n",
      "  -1.849685262091882e-12 -1.6779371534843638e-12 6.28680640343573e-12]\n",
      " [-9.173110020912043e-12 -8.553463868183318e-12 2.7849101008925218e-09\n",
      "  ... 6.061472386004574e-13 -1.5368483575650938e-11\n",
      "  -2.0799996365732506e-12]\n",
      " [-1.339721469366597e-12 -1.2473451736162118e-12 4.063718067714302e-10\n",
      "  ... -7.855888169873648e-12 -2.2424830660272505e-12\n",
      "  2.670035598702889e-11]\n",
      " ...\n",
      " [-1.7959997233526655e-11 -1.6736409547808716e-11 5.450568945442593e-09\n",
      "  ... 4.687030798680399e-10 -3.0078474423063527e-11\n",
      "  -1.593005943108984e-09]\n",
      " [-5.319942649327596e-13 -4.952669241693505e-13 1.6135876001646863e-10\n",
      "  ... 1.4092442123920463e-11 -8.904248743309896e-13\n",
      "  -4.789585006274799e-11]\n",
      " [-7.566710647532941e-12 -7.048353689901107e-12 2.2958269706982396e-09\n",
      "  ... 1.9707758191036562e-10 -1.266920308346659e-11\n",
      "  -6.698128944983418e-10]]\n",
      "[[9.840158133448103e-11 -7.608920117362387e-10]\n",
      " [9.753487240499585e-12 -7.574757278190703e-11]\n",
      " [-4.590478237498811e-09 3.553016855893957e-08]\n",
      " [1.132010791447608e-09 -8.751011920113205e-09]\n",
      " [4.416602943220886e-11 -3.4165835186498424e-10]\n",
      " [-1.2410399204981694e-12 9.50126633647253e-12]\n",
      " [6.155764845706782e-11 -4.761594421231554e-10]\n",
      " [-5.6724023292400435e-12 5.860082770355087e-11]\n",
      " [-3.03745505446073e-09 2.349333909800992e-08]\n",
      " [6.84341565281342e-10 -5.273689814316954e-09]\n",
      " [4.5823057657144406e-11 -3.5295504268981637e-10]\n",
      " [2.7253953256446973e-11 -2.1081313434215313e-10]\n",
      " [1.1083364276401394e-10 -8.571757984848321e-10]\n",
      " [5.128682818257692e-11 -3.968116151948734e-10]\n",
      " [7.735196828121827e-10 -5.963250726002758e-09]\n",
      " [-1.2131650644660656e-10 9.696900069644594e-10]\n",
      " [-2.2308634004748695e-12 1.715982506508157e-11]\n",
      " [3.292944310223077e-11 -2.5488026635125257e-10]\n",
      " [1.4987621410643537e-09 -1.15865128270399e-08]\n",
      " [4.087843439415641e-11 -3.162451357863403e-10]\n",
      " [-8.515092308163365e-09 6.585377714055164e-08]\n",
      " [-5.539237469327912e-09 4.285058446258978e-08]\n",
      " [-3.5202425200426934e-14 1.5342239966232945e-13]\n",
      " [-4.675567495018141e-09 3.616833249126882e-08]\n",
      " [4.380543273066669e-10 -3.386476607179803e-09]\n",
      " [1.1236338035676136e-10 -8.68818200918971e-10]\n",
      " [-1.720373979623533e-11 1.3279874622854032e-10]\n",
      " [-2.235340893507168e-09 1.7280240029689508e-08]\n",
      " [4.120857696189182e-09 -3.1851232242749643e-08]\n",
      " [-5.623820900525365e-09 4.347471245274209e-08]\n",
      " [6.488654536572864e-11 -5.016935491611923e-10]\n",
      " [1.3087560442723606e-10 -1.011731411224204e-09]\n",
      " [-1.517649997253473e-11 1.172548237599229e-10]\n",
      " [8.209010725909815e-11 -6.346631554727012e-10]]\n",
      "[[-1.746550864170041e-08]\n",
      " [1.50303296698671e-08]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'end' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-5ca4e2a2a2a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mderror_dwh5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mderror_dwo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Elapsed (with compilation) = %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'end' is not defined"
     ]
    }
   ],
   "source": [
    "#print(weight_hidden)\n",
    "print(lr * derror_dwh)\n",
    "print(lr * derror_dwh2)\n",
    "print(lr * derror_dwh3)\n",
    "print(lr * derror_dwh4)\n",
    "print(lr * derror_dwh5)\n",
    "print(lr * derror_dwo)\n",
    "print(\"Elapsed (with compilation) = %s\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003665061415010308\n"
     ]
    }
   ],
   "source": [
    "single_point = np.array(X_train)\n",
    "result1 = np.dot(single_point, weight_hidden)\n",
    "result2 = ReLU(result1)\n",
    "result3 = np.dot(result2, weight_hidden2)\n",
    "result4 = ReLU(result3)\n",
    "result5 = np.dot(result4, weight_hidden3)\n",
    "result6 = ReLU(result5)\n",
    "result7 = np.dot(result6, weight_hidden4)\n",
    "result8 = ReLU(result7)\n",
    "result9 = np.dot(result8, weight_hidden5)\n",
    "result10 = ReLU(result9)\n",
    "result11 = np.dot(result10, weight_output)\n",
    "result12 = ReLU(result11)\n",
    "#print(result12)\n",
    "#print(Y_train_random)\n",
    "error_out = ((1/len(Y_train)) * (np.power((result12 - Y_train), 2)))\n",
    "#print(error_out)\n",
    "print(error_out.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.87954086e-01]\n",
      " [ 9.83924495e-01]\n",
      " [ 8.92063090e-01]\n",
      " [ 9.65720949e-01]\n",
      " [ 2.14416410e-01]\n",
      " [ 1.01467068e+00]\n",
      " [-2.49431528e-03]\n",
      " [-1.20722218e-04]\n",
      " [ 1.01464735e+00]\n",
      " [ 9.70698361e-01]\n",
      " [-1.24641243e-03]\n",
      " [-1.31740138e-03]\n",
      " [ 1.04707058e+00]\n",
      " [-9.17969564e-04]\n",
      " [ 9.90399031e-01]\n",
      " [ 1.03755309e+00]\n",
      " [ 9.78691716e-01]\n",
      " [ 1.22296959e+00]\n",
      " [ 1.02181430e+00]\n",
      " [ 5.58355051e-01]\n",
      " [ 1.19906537e+00]\n",
      " [ 1.12365652e+00]\n",
      " [ 1.04530114e+00]\n",
      " [ 1.29632026e+00]\n",
      " [ 7.94258253e-01]\n",
      " [-5.59956176e-03]\n",
      " [-4.69200532e-03]\n",
      " [ 3.32417664e-01]\n",
      " [-5.29427118e-04]\n",
      " [-1.72817741e-03]\n",
      " [ 9.93157391e-01]\n",
      " [ 9.48957579e-01]\n",
      " [ 1.02590147e+00]\n",
      " [ 9.84630120e-01]\n",
      " [-1.36668590e-03]\n",
      " [-3.03370169e-03]\n",
      " [-1.84963106e-03]\n",
      " [-9.26873940e-04]\n",
      " [ 1.99991467e-01]\n",
      " [ 9.56344749e-01]\n",
      " [ 9.85186546e-01]\n",
      " [ 9.80830228e-01]\n",
      " [ 1.00297107e+00]\n",
      " [ 7.36648956e-01]\n",
      " [-1.19374586e-03]\n",
      " [ 9.28785240e-01]\n",
      " [-2.64875672e-03]\n",
      " [ 1.17441074e+00]\n",
      " [ 9.30307692e-01]\n",
      " [ 1.04147724e+00]\n",
      " [ 9.25904427e-01]\n",
      " [ 1.00118704e+00]\n",
      " [ 8.45170110e-01]\n",
      " [ 8.15218548e-01]\n",
      " [ 8.13361454e-01]\n",
      " [ 3.40887766e-01]\n",
      " [-9.41055543e-04]\n",
      " [ 5.73088060e-01]\n",
      " [-1.07023490e-03]\n",
      " [ 9.51771915e-01]\n",
      " [-6.36603722e-04]\n",
      " [-3.74336877e-03]\n",
      " [ 5.94530920e-01]\n",
      " [-1.84901160e-03]\n",
      " [ 1.00547443e+00]\n",
      " [ 1.15015985e+00]\n",
      " [ 5.93139355e-01]\n",
      " [ 2.52140482e-01]\n",
      " [ 3.22042514e-01]\n",
      " [-4.31468315e-03]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "70\n",
      "0.3131150547106697\n"
     ]
    }
   ],
   "source": [
    "single_point = np.array(X_test)\n",
    "result1 = np.array(np.dot(single_point, weight_hidden), dtype = np.float )\n",
    "result2 = ReLU(result1)\n",
    "result3 = np.array(np.dot(result2, weight_hidden2), dtype = np.float )\n",
    "result4 = ReLU(result3)\n",
    "result5 = np.array(np.dot(result4, weight_hidden3), dtype = np.float )\n",
    "result6 = ReLU(result5)\n",
    "result7 = np.array(np.dot(result6, weight_hidden4), dtype = np.float )\n",
    "result8 = ReLU(result7)\n",
    "result9 = np.array(np.dot(result8, weight_hidden5), dtype = np.float )\n",
    "result10 = ReLU(result9)\n",
    "result11 = np.array(np.dot(result10, weight_output), dtype = np.float )\n",
    "result12 = ReLU(result11)\n",
    "print(result12)\n",
    "print(Y_test)\n",
    "print(len(X_test))\n",
    "error_out = ((1/len(X_test)) * (np.power((result12 - Y_test), 2)))\n",
    "print(math.sqrt(error_out.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_point = np.array(X_train)\n",
    "result1 = np.dot(single_point, mbwh1)\n",
    "result2 = ReLU(result1)\n",
    "result3 = np.dot(result2, mbwh2)\n",
    "result4 = ReLU(result3)\n",
    "result5 = np.dot(result4, mbwh3)\n",
    "result6 = ReLU(result5)\n",
    "result7 = np.dot(result6, mbwh4)\n",
    "result8 = ReLU(result7)\n",
    "result9 = np.dot(result8, mbwh5)\n",
    "result10 = ReLU(result9)\n",
    "result11 = np.dot(result10, mbwo)\n",
    "result12 = ReLU(result11)\n",
    "print(\"bestloss1 = \", bestloss1)\n",
    "#print(result12)\n",
    "#print(Y_train_random)\n",
    "error_out = ((1/len(Y_train)) * (np.power((result12 - Y_train), 2)))\n",
    "print(error_out.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
